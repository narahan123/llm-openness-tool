import json
import re
import requests
import os
from pathlib import Path
from urllib.parse import urlparse
from dotenv import load_dotenv
from openai import OpenAI

from huggingface_Fetcher import huggingface_fetcher
from github_Fetcher import github_fetcher
from arxiv_Fetcher import arxiv_fetcher_from_model
# from openness_Evaluator import evaluate_openness
from github_Dispatcher import filter_github_features
from arxiv_Dispatcher import filter_arxiv_features
from huggingface_Dispatcher import filter_hf_features
from openness_Evaluator import evaluate_openness_from_files
from inference import run_inference

import html

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ë° OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
dotenv_path = os.path.join(os.getcwd(), '.env')
load_dotenv(dotenv_path)
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# GPT-4oë¡œ í”„ë¦¬íŠ¸ë ˆì¸(base) ëª¨ë¸ ì¶”ì •
def gpt_detect_base_model(hf_id: str) -> str | None:
    """
    â€¢ ì…ë ¥ ëª¨ë¸ì´ íŒŒì¸íŠœë‹ ëª¨ë¸ì´ë©´ â†’ í”„ë¦¬íŠ¸ë ˆì¸ ëª¨ë¸ ID ë°˜í™˜
    â€¢ ì´ë¯¸ í”„ë¦¬íŠ¸ë ˆì¸(=ì›ë³¸) ëª¨ë¸ì´ë©´ â†’ None ë°˜í™˜
    â€¢ í™•ì‹  ì—†ë‹¤ë©´ GPTê°€ null ë°˜í™˜ â†’ None
    """
    import textwrap

    def _hf_card_readme(mid: str, max_len: int = 12000) -> str:
        try:
            card = requests.get(
                f"https://huggingface.co/api/models/{mid}?full=true"
            ).json().get("cardData", {}) or {}
            txt = (card.get("content") or "")[:max_len]
            for br in ["main", "master"]:
                r = requests.get(f"https://huggingface.co/{mid}/raw/{br}/README.md")
                if r.status_code == 200:
                    txt += "\n\n" + r.text[:max_len]
                    break
            return txt
        except Exception:
            return ""

    prompt_sys = textwrap.dedent(f"""
        ë‹¹ì‹ ì€ AI ëª¨ë¸ ì •ë³´ë¥¼ ë¶„ì„í•´ â€˜í”„ë¦¬íŠ¸ë ˆì¸(ì›) ëª¨ë¸â€™ì„ ì°¾ì•„ë‚´ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

        â€¢ ì…ë ¥ ëª¨ë¸ **{hf_id}** ì´(ê°€) íŒŒì¸íŠœë‹ ëª¨ë¸ì¼ì§€ ëª¨ë¦…ë‹ˆë‹¤.
        â€¢ ì•„ë˜ì— ì œê³µë˜ëŠ” Hugging Face ì¹´ë“œ / README ë‚´ìš©ì„ ì½ê³ 
          â¡ï¸ íŒŒìƒë˜ì—ˆì„ ê°€ëŠ¥ì„±ì´ ê°€ì¥ ë†’ì€ í”„ë¦¬íŠ¸ë ˆì¸ ëª¨ë¸ IDë¥¼ ì¶”ì •í•˜ì‹­ì‹œì˜¤.
        â€¢ ì´ë¯¸ í”„ë¦¬íŠ¸ë ˆì¸ ëª¨ë¸ì´ë©´ nullì„ ë°˜í™˜í•˜ì‹­ì‹œì˜¤.

        â¤ ì¶œë ¥ í˜•ì‹ â†’ JSON í•œ ì¤„ë§Œ! ì˜ˆ:
            {{ "pretrain_model": "bigscience/bloom-560m" }}
          ë˜ëŠ”
            {{ "pretrain_model": null }}
    """).strip()

    ctx = _hf_card_readme(hf_id)
    if not ctx:
        return None

    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            response_format={"type": "json_object"},
            messages=[
                {"role": "system", "content": prompt_sys},
                {"role": "user",   "content": ctx}
            ],
            temperature=0
        )
        pred = json.loads(resp.choices[0].message.content)
        pre_id = pred.get("pretrain_model")
        # ê²€ì¦: ì¡´ì¬í•˜ê³  ì…ë ¥ê³¼ ë‹¤ë¥¼ ë•Œë§Œ ì‚¬ìš©
        if pre_id and isinstance(pre_id, str):
            pre_id = pre_id.strip()
            if pre_id.lower() != hf_id.lower() and test_hf_model_exists(pre_id):
                return pre_id
    except Exception as e:
        print("âš ï¸ GPT í”„ë¦¬íŠ¸ë ˆì¸ íƒì§€ ì‹¤íŒ¨:", e)
    return None
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


# 1. ì…ë ¥ íŒŒì‹±: URL ë˜ëŠ” org/model
def extract_model_info(input_str: str) -> dict:
    platform = None
    organization = model = None
    if input_str.startswith("http"):
        parsed = urlparse(input_str)
        domain = parsed.netloc.lower()
        segments = parsed.path.strip("/").split("/")
        if len(segments) >= 2:
            organization = segments[0]
            model = segments[1].split("?")[0].split("#")[0].replace(".git", "")
            if "huggingface" in domain:
                platform = "huggingface"
            elif "github" in domain:
                platform = "github"
    else:
        parts = input_str.strip().split("/")
        if len(parts) == 2:
            organization, model = parts
            platform = "unknown"
    if not organization or not model:
        raise ValueError("ì˜¬ë°”ë¥¸ ì…ë ¥ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤. 'org/model' ë˜ëŠ” URLì„ ì…ë ¥í•˜ì„¸ìš”.")
    full_id = f"{organization}/{model}"
    hf_id = full_id.lower()
    return {"platform": platform, "organization": organization,
            "model": model, "full_id": full_id, "hf_id": hf_id}

# 2. ì¡´ì¬ ì—¬ë¶€ í…ŒìŠ¤íŠ¸
def test_hf_model_exists(model_id: str) -> bool:
    resp = requests.get(f"https://huggingface.co/api/models/{model_id}")
    return resp.status_code == 200

def test_github_repo_exists(repo: str) -> bool:
    resp = requests.get(f"https://api.github.com/repos/{repo}")
    return resp.status_code == 200

# 3. ë§í¬ íŒŒì‹± í—¬í¼ (â˜… ì›ë¬¸ ì¼€ì´ìŠ¤ ìœ ì§€)
def extract_repo_from_url(url: str) -> str:
    parsed = urlparse(url)
    parts = parsed.path.strip("/").split("/")
    if len(parts) >= 2:
        repo = parts[1].split("?")[0].split("#")[0].replace(".git", "")
        return f"{parts[0]}/{repo}"  # ì›ë¬¸ ì¼€ì´ìŠ¤ ìœ ì§€
    return ""

# 4. HF í˜ì´ì§€ â†’ GitHub (ì—¬ëŸ¬ í›„ë³´ ìˆ˜ì§‘ â†’ ì†Œë¬¸ì ë¹„êµ/ì ìˆ˜í™” â†’ ì›ë¬¸ìœ¼ë¡œ ë°˜í™˜)
def find_github_in_huggingface(model_id: str) -> str | None:
    """
    HF ëª¨ë¸ ì¹´ë“œì—ì„œ GitHub ë§í¬ í›„ë³´ë“¤ì„ ì „ë¶€ ëª¨ì•„ ì ìˆ˜í™” í›„ ê°€ì¥ ê·¸ëŸ´ë“¯í•œ ë ˆí¬ë¥¼ ë°˜í™˜.
    ë¹„êµ/ì ìˆ˜í™”ëŠ” ì†Œë¬¸ì ê¸°ì¤€, ë°˜í™˜ì€ ì›ë¬¸ ì¼€ì´ìŠ¤ ê·¸ëŒ€ë¡œ.
    """
    def _extract_repo_from_url_preserve(url: str) -> str | None:
        try:
            p = urlparse(url)
            if "github.com" not in p.netloc.lower():
                return None
            seg = p.path.strip("/").split("/")
            if len(seg) >= 2:
                repo = seg[1].split("?")[0].split("#")[0].replace(".git", "")
                return f"{seg[0]}/{repo}"  # ì›ë¬¸ ì¼€ì´ìŠ¤ ìœ ì§€
        except Exception:
            pass
        return None

    def _tokenize(s: str) -> list[str]:
        s = s.lower().replace("/", " ")
        s = re.sub(r"[^a-z0-9]+", " ", s)
        return [t for t in s.split() if t]

    def _score(repo_lower: str, hf_org: str, toks: list[str]) -> int:
        score = 0
        org, name = repo_lower.split("/", 1)
        if hf_org and org == hf_org.lower():
            score += 5
        for t in toks:
            if t and t in name:
                score += 2
        if any(k in name for k in ["model", "models", "llm"]):
            score += 2
        for k in ["api","client","sdk","demo","website","docs","doc","notebook","colab",
                  "examples","sample","bench","leaderboard","eval","evaluation","convert",
                  "export","deploy","inference","space","slim","angelslim"]:
            if k in name:
                score -= 2
        return score

    try:
        card = requests.get(
            f"https://huggingface.co/api/models/{model_id}?full=true"
        ).json().get("cardData", {}) or {}

        hf_org = model_id.split("/")[0] if "/" in model_id else ""
        toks = _tokenize(model_id)

        # lower â†’ original ë§¤í•‘
        cand_map: dict[str, str] = {}

        def _add_candidate(rep: str | None):
            if not rep:
                return
            cand_map.setdefault(rep.lower(), rep)  # lower í‚¤ë¡œ dedup, ê°’ì€ ì›ë¬¸

        # 1) links.repository / links.homepage
        for field in ["repository", "homepage"]:
            links = (card.get("links", {}) or {}).get(field)
            if isinstance(links, str):
                _add_candidate(_extract_repo_from_url_preserve(links))
            elif isinstance(links, list):
                for u in links:
                    _add_candidate(_extract_repo_from_url_preserve(str(u)))

        # 2) model card content
        content = card.get("content", "") or ""
        for url in re.findall(r"https://github\.com/[A-Za-z0-9_.-]+/[A-Za-z0-9_.-]+", content):
            _add_candidate(_extract_repo_from_url_preserve(url))

        # 3) raw README
        for br in ["main", "master"]:
            try:
                r = requests.get(f"https://huggingface.co/{model_id}/raw/{br}/README.md")
                if r.status_code == 200:
                    for url in re.findall(r"https://github\.com/[A-Za-z0-9_.-]+/[A-Za-z0-9_.-]+", r.text):
                        _add_candidate(_extract_repo_from_url_preserve(url))
                    break
            except Exception:
                pass

        if not cand_map:
            return None

        # 4) ì¡´ì¬í•˜ëŠ” í›„ë³´ë§Œ ì ìˆ˜í™”(ë¹„êµëŠ” lower), ë°˜í™˜ì€ ì›ë¬¸ ì¼€ì´ìŠ¤
        best_lower, best_score = None, -10**9
        for rep_lower, rep_orig in cand_map.items():
            if not test_github_repo_exists(rep_orig):  # ì›ë¬¸ìœ¼ë¡œ ì¡´ì¬ í™•ì¸
                continue
            s = _score(rep_lower, hf_org, toks)
            if s > best_score:
                best_lower, best_score = rep_lower, s

        return cand_map[best_lower] if best_lower else None
    except Exception:
        return None

# 5. GitHub í˜ì´ì§€ â†’ HF (fallback: raw README and HTML)
def find_huggingface_in_github(repo: str) -> str:
    for fname in ["README.md"]:
        for branch in ["main", "master"]:
            raw_url = f"https://raw.githubusercontent.com/{repo}/{branch}/{fname}"
            try:
                r = requests.get(raw_url)
                if r.status_code == 200:
                    m = re.search(r"https?://huggingface\.co/([\w\-]+/[\w\-\.]+)", r.text, re.IGNORECASE)
                    if m:
                        candidate = m.group(1).lower()
                        if not candidate.startswith('collections/'):
                            return candidate
                    m2 = re.search(r"huggingface\.co/([\w\-]+/[\w\-\.]+)", r.text, re.IGNORECASE)
                    if m2:
                        candidate = m2.group(1).lower()
                        if not candidate.startswith('collections/'):
                            return candidate
                    m_md = re.search(r"\\[.*?\\]\\((https?://huggingface\.co/[\w\-/\.]+)\\)", r.text, re.IGNORECASE)
                    if m_md:
                        candidate = extract_model_info(m_md.group(1))["hf_id"]
                        if not candidate.startswith('collections/'):
                            return candidate
                    m_html = re.search(r'<a\\s+href="https?://huggingface\.co/([\w\-/\.]+)"', r.text, re.IGNORECASE)
                    if m_html:
                        candidate = m_html.group(1).lower()
                        if not candidate.startswith('collections/'):
                            return candidate
            except:
                pass
    try:
        html = requests.get(f"https://github.com/{repo}").text
        m3 = re.findall(r"https://huggingface\.co/[\w\-]+/[\w\-\.]+", html, re.IGNORECASE)
        for link in m3:
            if 'href' in html[html.find(link)-20:html.find(link)]:
                candidate = extract_model_info(link)["hf_id"]
                if not candidate.startswith('collections/'):
                    return candidate
    except:
        pass
    return None

def gpt_guess_github_from_huggingface(hf_id: str) -> str:
    prompt = f"""
Hugging Faceì— ë“±ë¡ëœ ëª¨ë¸ '{hf_id}'ì— ëŒ€í•´, ì´ ëª¨ë¸ì˜ ì›ë³¸ ì½”ë“œê°€ ì €ì¥ëœ GitHub ì €ì¥ì†Œë¥¼ ì¶”ì •í•˜ì„¸ìš”.

ğŸŸ¢ ì§€ì¼œì•¼ í•  ê·œì¹™:
1. 'organization/repo' í˜•ì‹ìœ¼ë¡œ **ì •í™•í•œ GitHub ê²½ë¡œë§Œ** ë°˜í™˜í•˜ì„¸ìš” (ë§í¬ X, ì„¤ëª… X).
2. 'google-research/google-research'ì²˜ëŸ¼ ë„ˆë¬´ ì¼ë°˜ì ì¸ ëª¨ë…¸ë¦¬í¬ì§€í„°ë¦¬ëŠ” í”¼í•˜ê³ , ëª¨ë¸ ë‹¨ìœ„ ì €ì¥ì†Œê°€ ìˆë‹¤ë©´ ê·¸ê²ƒì„ ìš°ì„  ì¶”ì •í•˜ì„¸ìš”.
3. distill ëª¨ë¸ì¸ ê²½ìš° ë¶€ëª¨ ëª¨ë¸ì„ ì°¾ì•„ì£¼ì„¸ìš”.
4. í•´ë‹¹ ëª¨ë¸ì˜ ì´ë¦„, êµ¬ì¡°, ë…¼ë¬¸, tokenizer, ì‚¬ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬(PyTorch, JAX, T5 ë“±)ë¥¼ ì°¸ê³ í•´ì„œ ì •í™•í•œ repoë¥¼ ì¶”ì •í•˜ì„¸ìš”.
5. ê²°ê³¼ëŠ” **ë”± í•œ ì¤„**, ì˜ˆ: `facebookresearch/llama`

ğŸ”´ ì¶œë ¥ì—ëŠ” ë¶€ê°€ ì„¤ëª… ì—†ì´ GitHub ì €ì¥ì†Œ ê²½ë¡œë§Œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
"""
    try:
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
        )
        guess = response.choices[0].message.content.strip()
        if "/" in guess:
            return guess
    except Exception as e:
        print("âš ï¸ GPT HFâ†’GH ì¶”ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ:", e)
    return None

def gpt_guess_huggingface_from_github(gh_id: str) -> str:
    prompt = f"""
Hugging Faceì— ë“±ë¡ëœ ëª¨ë¸ '{gh_id}'ì˜ ì›ë³¸ ì½”ë“œê°€ ì €ì¥ëœ Hugging Face ëª¨ë¸ IDë¥¼ ì¶”ì •í•´ì£¼ì„¸ìš”.
- ì •í™•í•œ organization/repository ê²½ë¡œë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš”.
- ëª¨ë¸ ì´ë¦„ì´ë‚˜ ê´€ë ¨ ë…¼ë¬¸ì—ì„œ ìœ ë˜ëœ GitHub ì €ì¥ì†Œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¶”ì •í•˜ì„¸ìš”.
- ì˜ˆì‹œ ì¶œë ¥: facebookresearch/llama
- 'google-research/google-research'ì²˜ëŸ¼ ê´‘ë²”ìœ„í•œ ëª¨ë…¸ë¦¬í¬ì§€í„°ë¦¬ëŠ” í”¼í•˜ê³ , í•´ë‹¹ ëª¨ë¸ì„ ìœ„í•œ ë³„ë„ ì €ì¥ì†Œê°€ ìˆë‹¤ë©´ ê·¸ìª½ì„ ìš°ì„  ê³ ë ¤í•˜ì„¸ìš”.
"""
    try:
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
        )
        guess = response.choices[0].message.content.strip().lower()
        if "/" in guess:
            return guess
    except Exception as e:
        print("âš ï¸ GPT GHâ†’HF ì¶”ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ:", e)
    return None

def run_all_fetchers(user_input: str):
    outdir = make_model_dir(user_input)
    print(f"ğŸ“ ì¶œë ¥ ê²½ë¡œ: {outdir}")
    info = extract_model_info(user_input)
    hf_id = gh_id = None
    found_rank_hf = found_rank_gh = None
    full = info['full_id']
    hf_cand = info['hf_id']

    hf_ok = test_hf_model_exists(hf_cand)
    gh_ok = test_github_repo_exists(full)
    print(f"1ï¸âƒ£ HF: {hf_ok}, GH: {gh_ok}")

    if hf_ok:
        hf_id = hf_cand
        found_rank_hf = 1
    if gh_ok:
        gh_id = full
        found_rank_gh = 1

    if hf_ok and not gh_id:
        gh_link = find_github_in_huggingface(hf_cand)
        print(f"ğŸ” 2ìˆœìœ„ HFâ†’GH link: {gh_link}")
        if gh_link and test_github_repo_exists(gh_link):  # ì›ë¬¸ ì¼€ì´ìŠ¤ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            gh_id = gh_link
            found_rank_gh = 2

    if gh_ok and not hf_id:
        hf_link = find_huggingface_in_github(full)
        print(f"ğŸ” 2ìˆœìœ„ GHâ†’HF link: {hf_link}")
        if hf_link and test_hf_model_exists(hf_link):
            hf_id = hf_link
            found_rank_hf = 2

    if hf_ok and not gh_id:
        guess_gh = gpt_guess_github_from_huggingface(hf_cand)
        print(f"â³ 3ìˆœìœ„ GPT HFâ†’GH guess: {guess_gh}")
        if guess_gh and test_github_repo_exists(guess_gh):
            gh_id = guess_gh
            found_rank_gh = 3
            print("âš ï¸ GPT ì¶”ì • ê²°ê³¼ì…ë‹ˆë‹¤. ì €ì¥ì†Œê°€ ì‹¤ì œë¡œ í•´ë‹¹ ëª¨ë¸ì„ í¬í•¨í•˜ëŠ”ì§€ ê²€í†  í•„ìš”")

    if gh_ok and not hf_id:
        guess_hf = gpt_guess_huggingface_from_github(full)
        print(f"â³ 3ìˆœìœ„ GPT GHâ†’HF guess: {guess_hf}")
        if guess_hf and test_hf_model_exists(guess_hf):
            hf_id = guess_hf
            found_rank_hf = 3
            print("âš ï¸ GPT ì¶”ì • ê²°ê³¼ì…ë‹ˆë‹¤. ëª¨ë¸ IDê°€ ì •í™•í•œì§€ ê²€í†  í•„ìš”")

    if hf_id:
        rank_hf = found_rank_hf or 'ì—†ìŒ'
        print(f"âœ… HF model: {hf_id} (ë°œê²¬: {rank_hf}ìˆœìœ„)")
        data = huggingface_fetcher(hf_id, save_to_file=True, output_dir=outdir)
        arxiv_fetcher_from_model(hf_id, save_to_file=True, output_dir=outdir)
        try:
            hf_filtered = filter_hf_features(hf_id, output_dir=outdir)
        except FileNotFoundError:
            hf_filtered = {}
            print("âš ï¸ HuggingFace JSON íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šì•„ í•„í„°ë§ ìƒëµ")
        try:
            ax_filtered = filter_arxiv_features(hf_id, output_dir=outdir)
        except FileNotFoundError:
            ax_filtered = {}
            print("âš ï¸ arXiv JSON íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šì•„ í•„í„°ë§ ìƒëµ")
    else:
        print("âš ï¸ HuggingFace ì •ë³´ ì—†ìŒ")
    if gh_id:
        rank_gh = found_rank_gh or 'ì—†ìŒ'
        print(f"âœ… GH repo: {gh_id} (ë°œê²¬: {rank_gh}ìˆœìœ„)")

        try:
            github_fetcher(gh_id, branch="main", save_to_file=True, output_dir=outdir)
        except requests.exceptions.HTTPError:
            print("âš ï¸ main ë¸Œëœì¹˜ ì ‘ê·¼ ì‹¤íŒ¨, master ë¸Œëœì¹˜ë¡œ ì¬ì‹œë„...")
            try:
                github_fetcher(gh_id, branch="master", save_to_file=True, output_dir=outdir)
            except Exception as e:
                print("âŒ master ë¸Œëœì¹˜ë„ ì‹¤íŒ¨:", e)

        try:
            gh_filtered = filter_github_features(gh_id, output_dir=outdir)
        except FileNotFoundError:
            gh_filtered = {}
            print("âš ï¸ GitHub JSON íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šì•„ í•„í„°ë§ ìƒëµ")
    else:
        print("âš ï¸ GitHub ì •ë³´ ì—†ìŒ")

    # â”€â”€â”€ GPT ê¸°ë°˜ í”„ë¦¬íŠ¸ë ˆì¸ ëª¨ë¸ íƒì§€ + íŒŒì´í”„ë¼ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    base_model_id = gpt_detect_base_model(hf_id) if hf_id else None
    if base_model_id:
        print(f"ğŸ§± GPTê°€ ì°¾ì€ í”„ë¦¬íŠ¸ë ˆì¸ ëª¨ë¸: {base_model_id}")

        # 1) Hugging Face fetch/dispatch
        huggingface_fetcher(base_model_id, save_to_file=True, output_dir=outdir)
        from pretrain_hf_Dispatcher import filter_pretrain_hf
        filter_pretrain_hf(base_model_id, output_dir=outdir)

        # 2) GitHub (ìˆì„ ë•Œë§Œ)
        base_gh = find_github_in_huggingface(base_model_id)
        if base_gh:
            try:
                github_fetcher(base_gh, save_to_file=True, output_dir=outdir)
                from pretrain_github_Dispatcher import filter_pretrain_gh
                filter_pretrain_gh(base_gh, output_dir=outdir)
            except Exception as e:
                print("âš ï¸ GH fetch/dispatch ì‹¤íŒ¨:", e)
        else:
            print("âš ï¸ í”„ë¦¬íŠ¸ë ˆì¸ ëª¨ë¸ì˜ GitHub ë ˆí¬ë¥¼ ì°¾ì§€ ëª»í•´ GH fetcher ê±´ë„ˆëœ€")

        # 3) arXiv (ìˆì„ ë•Œë§Œ)
        try:
            ax_ok = arxiv_fetcher_from_model(base_model_id,
                                             save_to_file=True,
                                             output_dir=outdir)
            if ax_ok:
                from pretrain_arxiv_Dispatcher import filter_pretrain_arxiv
                filter_pretrain_arxiv(base_model_id, output_dir=outdir)
            else:
                print("âš ï¸ ë…¼ë¬¸ ë§í¬ë¥¼ ì°¾ì§€ ëª»í•´ arXiv fetcher ê±´ë„ˆëœ€")
        except Exception as e:
            print("âš ï¸ arXiv fetch/dispatch ì‹¤íŒ¨:", e)
    else:
        base_model_id = None  # GPTê°€ null ë°˜í™˜ â†’ í”„ë¦¬íŠ¸ë ˆì¸ ì—†ìŒ


    # 8. Openness í‰ê°€ ìˆ˜í–‰
    try:
        print("ğŸ“ ê°œë°©ì„± í‰ê°€ ì‹œì‘...")
        eval_res = evaluate_openness_from_files(
            full,
            base_dir=str(outdir),
            base_model_id=base_model_id        # â† ì¸ì ì¶”ê°€
        )
        base = full.replace("/", "_")
        outfile = Path(outdir) / f"openness_score_{base}.json"
        print(f"âœ… ê°œë°©ì„± í‰ê°€ ì™„ë£Œ.  ê²°ê³¼ íŒŒì¼: {outfile}")
    except Exception as e:
        print("âš ï¸ ê°œë°©ì„± í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:", e)

    # README ê¸°ë°˜ ì¶”ë¡ ì€ dataê°€ ìˆì„ ë•Œë§Œ
    if 'data' in locals() and isinstance(data, dict) and data.get("readme"):
        run_inference(data.get("readme"))

def make_model_dir(user_input: str) -> Path:
    info = extract_model_info(user_input)
    base = info["hf_id"]                         # ex) 'bigscience/bloomz-560m'
    safe = re.sub(r'[<>:"/\\|?*\s]+', "_", base) # ex) 'bigscience_bloomz-560m'
    path = Path(safe)
    path.mkdir(parents=True, exist_ok=True)
    return path
###################################################################
# if __name__ == "__main__":
#     user_input = input("ğŸŒ HF/GH URL ë˜ëŠ” org/model: ").strip()
#     model_dir = make_model_dir(user_input)
#     print(f"ğŸ“ ìƒì„±/ì‚¬ìš©í•  í´ë”: {model_dir}")
#     run_all_fetchers(user_input)

#     info = extract_model_info(user_input)
#     hf_id = info['hf_id']

#     if test_hf_model_exists(hf_id):
#         with open(model_dir / "identified_model.txt", "w", encoding="utf-8") as f:
#             f.write(hf_id)
#         print(f"âœ… ëª¨ë¸ ID ì €ì¥ ì™„ë£Œ: {model_dir / 'identified_model.txt'}")
#######################################################################
if __name__ == "__main__":
    try:
        n = int(input("ğŸ”¢ ëŒë¦´ ëª¨ë¸ ê°œìˆ˜: ").strip())
    except ValueError:
        print("ìˆ«ìë¥¼ ì…ë ¥í•˜ì„¸ìš”."); exit(1)

    models: list[str] = []
    for i in range(1, n + 1):
        m = input(f"[{i}/{n}] ğŸŒ HF/GH URL ë˜ëŠ” org/model: ").strip()
        if m:
            models.append(m)

    print("\nğŸš€ ì´", len(models), "ê°œ ëª¨ë¸ì„ ìˆœì°¨ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n")

    for idx, user_input in enumerate(models, 1):
        print(f"\n======== {idx}/{len(models)} â–¶ {user_input} ========")
        try:
            model_dir = make_model_dir(user_input)
            print(f"ğŸ“ ìƒì„±/ì‚¬ìš©í•  í´ë”: {model_dir}")
            run_all_fetchers(user_input)

            info  = extract_model_info(user_input)
            hf_id = info["hf_id"]
            if test_hf_model_exists(hf_id):
                with open(model_dir / "identified_model.txt", "w", encoding="utf-8") as f:
                    f.write(hf_id)
                print(f"âœ… ëª¨ë¸ ID ì €ì¥ ì™„ë£Œ: {model_dir / 'identified_model.txt'}")

        except Exception as e:
            print("âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:", e)
            # ì—ëŸ¬ ë¡œê·¸ ë‚¨ê¸°ê³  ë‹¤ìŒ ëª¨ë¸ë¡œ ê³„ì†
            continue

    print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ëë‚¬ìŠµë‹ˆë‹¤.")


    # âœ… ë°”ë¡œ ì¶”ë¡ ê¹Œì§€ ì‹¤í–‰
    prompt = input("ğŸ“ í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")
    run_inference(hf_id, prompt)
