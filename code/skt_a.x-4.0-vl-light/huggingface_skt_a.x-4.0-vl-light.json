{
    "model_id": "skt/a.x-4.0-vl-light",
    "files": [
        ".gitattributes",
        "LICENSE",
        "README.md",
        "assets/A.X_logo_ko_4x3.png",
        "assets/benchmark_2x2.png",
        "assets/document.png",
        "assets/image.png",
        "chat_template.json",
        "config.json",
        "configuration_ax4vl.py",
        "generation_config.json",
        "image_processing_ax4vl.py",
        "merges.txt",
        "model-00001-of-00004.safetensors",
        "model-00002-of-00004.safetensors",
        "model-00003-of-00004.safetensors",
        "model-00004-of-00004.safetensors",
        "model.safetensors.index.json",
        "modeling_ax4vl.py",
        "preprocessor_config.json",
        "processing_ax4vl.py",
        "processor_config.json",
        "special_tokens_map.json",
        "tokenizer_config.json",
        "vocab.json"
    ],
    "readme": "---\nlicense: apache-2.0\nlicense_link: https://huggingface.co/skt/A.X-4.0-VL-Light/blob/main/LICENSE\nlanguage:\n- en\n- ko\npipeline_tag: image-text-to-text\nlibrary_name: transformers\nmodel_id: skt/A.X-4.0-VL-Light\ndevelopers: SKT AI Model Lab\nbase_model:\n- skt/A.X-4.0-Light\n---\n\n# A.X 4.0 VL Light\n\n<p align=\"center\">\n    <picture>\n        <img src=\"./assets/A.X_logo_ko_4x3.png\" width=\"45%\" style=\"margin: 40px auto;\">\n    </picture>\n</p>\n<p align=\"center\"> <a href=\"https://huggingface.co/collections/skt/ax-4-68637ebaa63b9cc51925e886\">ğŸ¤— Models</a>   |   <a href=\"https://github.com/SKT-AI/A.X-4.0-VL-Light\">ğŸ–¥ï¸ Github</a> </p>\n\n\n\n## Highlights \n\n**A.X 4.0 VL Light** (pronounced â€œA dot Xâ€) is a vision-language model (VLM) optimized for Korean vision and language understanding as well as enterprise deployment. Built upon [A.X 4.0 Light](https://huggingface.co/skt/A.X-4.0-Light), A.X 4.0 VL Light has been further trained on diverse multimodal datasets, with a particular focus on large-scale multimodal Korean datasets, to deliver exceptional performance in domestic business applications.\n\n- **Superior Korean Proficiency in Vision and Language**: Achieved an average score of 79.4 on Korean image benchmarks, outperforming Qwen2.5-VL-32B (73.4), despite having a significantly smaller model size. On Korean text benchmarks, recorded an average score of 60.2, comparable to VARCO-VISION-2.0-14B (60.4), while using only half the model size.\n- **Deep Cultural Understanding**: Scored 80.2 on K-Viscuit, a multimodal benchmark designed to evaluate cultural and contextual comprehension in Korean, exceeding Qwen2.5-VL-32B (72.3).\n- **Advanced Document Understanding**: Attained a score of 89.8 on KoBizDoc, a benchmark focused on understanding complex document structures, including charts and tables, performing comparably to Qwen2.5-VL-32B (88.8).\n- **Efficient Token Usage**: A.X 4.0 VL Light utilizes approximately 41% fewer text tokens compared to Qwen2.5-VL for the same Korean input, enabling significantly more cost-effective and efficient processing.\n\nA brief comparison on representative benchmarks is as follows:\n\n\n<p align=\"center\">\n    <picture>\n        <img src=\"./assets/benchmark_2x2.png\" width=\"80%\" style=\"margin: 40px auto;\">\n    </picture>\n</p>\n\n## Performance\n\n### Image Benchmark\n*Korean benchmarks, with K-Viscuit translated into Korean.\n\n| Category               | Benchmarks          | A.X 4.0 VL Light | Qwen2.5-VL-7B | InternVL3-8B | VARCO-VISION-2.0-14B | Qwen2.5-VL-32B |\n|------------------------|---------------------|------------------|---------------|--------------|----------------------|----------------|\n| Document               | KoBizDoc*           |             89.8 |          84.0 |         73.2 |                 83.0 |           88.8 |\n|                        | K-DTCBench*         |             90.0 |          86.7 |         83.8 |                 80.8 |           91.7 |\n|                        | ChartQA             |             79.8 |          80.6 |         79.8 |                 78.8 |           81.8 |\n|                        | DocVQA              |             94.4 |          95.3 |         92.4 |                 91.9 |           94.5 |\n|                        | InfoVQA             |             78.5 |          82.7 |         76.2 |                 80.0 |           82.7 |\n|                        | SEEDBench2-Plus     |             69.7 |          71.2 |         69.7 |                 71.9 |           73.3 |\n| OCR                    | OutdoorKorean*      |             97.3 |          91.9 |         72.7 |                 79.7 |           86.9 |\n|                        | K-Handwriting*      |             84.3 |          85.0 |         43.5 |                 55.2 |           60.1 |\n|                        | TextVQA             |             82.0 |          85.4 |         82.1 |                 80.3 |           79.8 |\n| Culture                | K-Viscuit*          |             80.2 |          65.0 |         65.3 |                 72.0 |           72.3 |\n| Knowledge              | KoEduBench*         |             58.1 |          53.9 |         53.9 |                 39.4 |           52.4 |\n|                        | KoCertBench*        |             54.9 |          50.1 |         39.4 |                 51.4 |           47.5 |\n|                        | MMMU                |             54.1 |          56.3 |         59.4 |                 58.3 |           63.6 |\n|                        | ScienceQA           |             95.3 |          87.2 |         97.8 |                 92.2 |           92.4 |\n| General                | K-LLaVA-W*          |             83.2 |          73.0 |         67.0 |                 80.0 |           84.3 |\n|                        | K-SEED*             |             76.5 |          76.4 |         76.4 |                 76.9 |           77.3 |\n|                        | SEEDBench_IMG       |             76.7 |          77.1 |         77.1 |                 78.1 |           77.6 |\n| Hallucination          | HallusionBench      |             54.2 |          52.7 |         49.6 |                 53.8 |           58.0 |\n| IF                     | MM-IFEval           |             53.5 |          51.4 |         51.9 |                 50.8 |           59.3 |\n\n\n\nThe following in-house benchmarks have been established to rigorously assess model performance on Korean vision-language understanding and the comprehension of Korea-specific knowledge domains:\n\n- **KoBizDoc**: A visual question answering (VQA) benchmark designed for understanding Korean business documents.\n- **OutdoorKorean**: A benchmark focused on recognizing Korean text in complex outdoor scenes (provided by AIHub).\n- **K-Handwriting**: A Korean handwriting recognition dataset comprising various handwritten styles (provided by AIHub).\n- **KoEduBench**: A VQA benchmark targeting Korean general academic exams, including GED and CSAT questions, to assess academic reasoning ability.\n- **KoCertBench**: A Korean certification exam-based VQA benchmark, covering domains such as civil service, technical licenses, and professional qualifications.\n\n### Text Benchmark\n*Korean benchmarks.\n\n| Category              | Benchmarks   | A.X 4.0 VL Light | Qwen2.5-VL-7B | InternVL3-8B | VARCO-VISION-2.0-14B |\n|-----------------------|--------------|------------------|---------------|--------------|----------------------|\n| Knowledge             | KMMLU*       |             60.5 |          45.6 |         50.9 |                 58.8 |\n|                       | MMLU         |             72.6 |          71.9 |         77.5 |                 80.7 |\n| Math                  | HRM8K*       |             40.6 |          25.4 |         34.6 |                 49.5 |\n|                       | MATH         |             56.5 |          61.7 |         65.1 |                 71.1 |\n| General               | Ko-MT-bench* |             68.9 |          51.5 |         59.5 |                 75.9 |\n|                       | MT-bench     |             72.9 |          73.2 |         69.9 |                 76.6 |\n| IF                    | Ko-IFEval*   |             71.8 |          55.0 |         46.1 |                 57.2 |\n|                       | IFEval       |             81.9 |          66.6 |         67.5 |                 75.3 |\n\n\n\n\n## ğŸš€ Quickstart\n\n### with HuggingFace Transformers\n\n- `transformers>=4.49.0` or the latest version is required to use `skt/A.X-4.0-VL-Light`\n\n```bash\npip install transformers>=4.49.0\n```\n\n#### Example Usage\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoProcessor\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\n\n\nmodel_name = \"skt/A.X-4.0-VL-Light\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, torch_dtype=torch.bfloat16).to(device='cuda')\nprocessor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n\nurl = \"https://huggingface.co/skt/A.X-4.0-VL-Light/resolve/main/assets/image.png\"\n# ì´ë¯¸ì§€ ì¶œì²˜: êµ­ê°€ìœ ì‚°í¬í„¸ (https://www.heritage.go.kr/unisearch/images/national_treasure/thumb/2021042017434700.JPG)\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nimage = Image.open(BytesIO(response.content))\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\"},\n            {\"type\": \"text\", \"text\": \"ì´ë¯¸ì§€ì— ëŒ€í•´ì„œ ì„¤ëª…í•´ì¤˜.\"},\n        ],\n    }\n]\n\ninputs = processor(\n    images=[image],\n    conversations=[messages],\n    padding=True,\n    return_tensors=\"pt\",\n).to(\"cuda\")\n\n# Decoding parameters (top_p, temperature, top_k, repetition_penalty) should be tuned depending on the generation task.\ngeneration_kwargs = {\n    \"max_new_tokens\": 256,\n    \"top_p\": 0.8,\n    \"temperature\": 0.5,\n    \"top_k\": 20,\n    \"repetition_penalty\": 1.05,\n    \"do_sample\": True,\n}\ngenerated_ids = model.generate(**inputs, **generation_kwargs)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\nresponse = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(response[0])\n\"\"\"\nìˆ­ë¡€ë¬¸ì€ ëŒ€í•œë¯¼êµ­ ì„œìš¸ì— ìœ„ì¹˜í•œ êµ­ë³´ ì œ1í˜¸ë¡œ, ì¡°ì„  ì‹œëŒ€ì— ê±´ì¶•ëœ ëª©ì¡° ê±´ì¶•ë¬¼ì´ë‹¤. ì´ ë¬¸ì€ ì„œìš¸ì˜ ë‚¨ìª½ ëŒ€ë¬¸ìœ¼ë¡œ, ì „í†µì ì¸ í•œêµ­ ê±´ì¶• ì–‘ì‹ì„ ë³´ì—¬ì¤€ë‹¤. ë‘ ì¸µìœ¼ë¡œ ì´ë£¨ì–´ì§„ ì´ ë¬¸ì€ ê¸°ì™€ì§€ë¶•ì„ ì–¹ê³  ìˆìœ¼ë©°, ì§€ë¶•ì˜ ê³¡ì„ ì´ ì•„ë¦„ë‹µê²Œ í‘œí˜„ë˜ì–´ ìˆë‹¤. ë¬¸ ì•„ë˜ì—ëŠ” ì•„ì¹˜í˜•ì˜ ì¶œì…êµ¬ê°€ ìˆìœ¼ë©°, ê·¸ ì£¼ìœ„ë¡œëŠ” ê²¬ê³ í•œ ì„ì¬ë¡œ ìŒ“ì€ ì„±ë²½ì´ ì´ì–´ì ¸ ìˆë‹¤. ë°°ê²½ì—ëŠ” í˜„ëŒ€ì ì¸ ê³ ì¸µ ë¹Œë”©ë“¤ì´ ìë¦¬ì¡ê³  ìˆì–´, ì „í†µê³¼ í˜„ëŒ€ê°€ ê³µì¡´í•˜ëŠ” ì„œìš¸ì˜ ëª¨ìŠµì„ ì˜ ë‚˜íƒ€ë‚¸ë‹¤. ìˆ­ë¡€ë¬¸ì€ ì—­ì‚¬ì , ë¬¸í™”ì  ê°€ì¹˜ê°€ ë†’ì•„ ë§ì€ ê´€ê´‘ê°ë“¤ì´ ì°¾ëŠ” ëª…ì†Œì´ë‹¤.\n\"\"\"\n\n```\n\n\n#### Example for Document Transcription\n\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoProcessor\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\n\n\nmodel_name = \"skt/A.X-4.0-VL-Light\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, torch_dtype=torch.bfloat16).to(device='cuda')\nprocessor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n\nurl = \"https://huggingface.co/skt/A.X-4.0-VL-Light/resolve/main/assets/document.png\"\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nimage = Image.open(BytesIO(response.content))\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\"},\n            {\"type\": \"text\", \"text\": \"ì‚¬ì§„ì— ë¬´ì—‡ì´ ì í˜€ìˆë‚˜ìš”? ë‹¤ë¥¸ ì„¤ëª… ì—†ì´ ì í˜€ìˆëŠ” í…ìŠ¤íŠ¸ë§Œ ê²°ê³¼ë¡œ ë³´ì—¬ì¤˜.\"},\n        ],\n    }\n]\n\ninputs = processor(\n    images=[image],\n    conversations=[messages],\n    padding=True,\n    return_tensors=\"pt\",\n).to(\"cuda\")\n\n\ngeneration_kwargs = {\n    \"max_new_tokens\": 1024,\n    \"top_p\": 0.95,\n    \"top_k\": 1,\n    \"temperature\": 0.7,\n    \"repetition_penalty\": 1.05,\n    \"do_sample\": True,\n}\ngenerated_ids = model.generate(**inputs, **generation_kwargs)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\nresponse = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(response[0])\n\"\"\"\n# A.X 4.0: ê¸°ì—…ìš© í•œêµ­ì–´ íŠ¹í™” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸\n\nView English README\n\nSKí…”ë ˆì½¤ì´ í•œêµ­ì–´ ì²˜ë¦¬ ëŠ¥ë ¥ê³¼ ê¸°ì—… í™œìš©ì„±ì„ ë†’ì¸ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM) A.X 4.0 (ì—ì´ë‹·ì—‘ìŠ¤ 4.0)ì„ 2025ë…„ 4ì›” 30ì¼ì— ì¶œì‹œí•˜ì˜€ìŠµë‹ˆë‹¤. A.X 4.0ì€ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ì¸ Qwen2.5ì— ë°©ëŒ€í•œ í•œêµ­ì–´ ë°ì´í„°ë¥¼ ì¶”ê°€ë¡œ í•™ìŠµì‹œì¼œ êµ­ë‚´ ë¹„ì¦ˆë‹ˆìŠ¤ í™˜ê²½ì— ìµœì í™”ëœ ì„±ëŠ¥ì„ ë°œíœ˜í•©ë‹ˆë‹¤.\n\n## A.X 4.0, ë¬´ì—‡ì´ ë‹¤ë¥¸ê°€ìš”?\n\n- ë›°ì–´ë‚œ í•œêµ­ì–´ ì‹¤ë ¥: ëŒ€í‘œì ì¸ í•œêµ­ì–´ ëŠ¥ë ¥ í‰ê°€ ë²¤ì¹˜ë§ˆí¬ì¸ KMMLUì—ì„œ 78.3ì ì„ ê¸°ë¡í•˜ì—¬, GPT-40(72.5ì )ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.\n- ë†’ì€ í•œêµ­ ë¬¸í™” ì´í•´ë„: í•œêµ­ì–´ ë° í•œêµ­ ë¬¸í™” ë²¤ì¹˜ë§ˆí¬ì¸ CLiCkì—ì„œë„ 83.5ì ì„ íšë“í•´, GPT-40(80.2ì )ë³´ë‹¤ ë” ë†’ì€ ì´í•´ë„ë¥¼ ì…ì¦í–ˆìŠµë‹ˆë‹¤.\n- íš¨ìœ¨ì ì¸ í† í° ì²˜ë¦¬: ë™ì¼í•œ í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ë„ A.X 4.0ë³´ë‹¤ GPT-40ê°€ ì•½ 1.5ë°° ë§ì€ í† í°ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n- ë°©ëŒ€í•œ ì •ë³´ ì²˜ë¦¬: ìµœëŒ€ 131,072 í† í°ì— ì´ë¥´ëŠ” ê¸´ ë¬¸ì„œë‚˜ ëŒ€í™”ë„ í•œ ë²ˆì— ì´í•´í•˜ê³  ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n- ë„ë©”ì¸ ì§€ì›: ì½”ë”©, ì œì¡°ì—… ë“± ì „ë¬¸ ì§€ì‹ì´ í•„ìš”í•œ ë¶„ì•¼ì—ì„œë„ í™œìš©í•  ìˆ˜ ìˆë„ë¡ ê¸°ë³¸ ì„±ëŠ¥ì„ ê°•í™”í–ˆìŠµë‹ˆë‹¤.\n- ë°°í¬ ì˜µì…˜: 720ì–µ ê°œ(72B) ë§¤ê°œë³€ìˆ˜ë¥¼ ê°–ì¶˜ í‘œì¤€ ëª¨ë¸ê³¼ 70ì–µ ê°œ(7B) ë§¤ê°œë³€ìˆ˜ì˜ ê²½ëŸ‰ ëª¨ë¸ë¡œ ì œê³µë˜ë©°, ê¸°ì—… ë‚´ë¶€ ì„œë²„ì— ì§ì ‘ ì„¤ì¹˜(ì˜¨í”„ë ˆë¯¸ìŠ¤)í•  ìˆ˜ ìˆì–´ ë°ì´í„° ë³´ì•ˆì— ëŒ€í•œ ê±±ì •ì„ ëœ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n## í•µì‹¬ ê¸°ìˆ ì€?\n\n### í•œêµ­ì–´ íŠ¹í™” í† í¬ë‚˜ì´ì € ì ìš©\n\ní•œêµ­ì–´ì˜ ê³ ìœ í•œ íŠ¹ì„±ì„ ì˜ ì´í•´í•˜ë„ë¡ ìµœì í™”ëœ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ í† í¬ë‚˜ì´ì €ëŠ” í•œêµ­ì–´ì˜ ë‹¤ì–‘í•œ í‘œí˜„ê³¼ ë¬¸ë§¥ì„ íš¨ê³¼ì ìœ¼ë¡œ íŒŒì•…í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ë‚´ë¶€ í…ŒìŠ¤íŠ¸ ê²°ê³¼, ê°™ì€ í•œêµ­ì–´ ë¬¸ì¥ì„ ì…ë ¥í–ˆì„ ë•Œ GPT-40ë³´ë‹¤ A.X 4.0ì´ 33.3% íš¨ìœ¨ì ìœ¼ë¡œ í† í°ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n\nì´ëŠ” ì‹¤ì œ ì‚¬ìš© í™˜ê²½ì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.\n\n- ê°™ì€ ì¡°ê±´ì´ë¼ë©´ ëŒ€ëµ 1.5ë°° ë” ë§ì€ í•œêµ­ì–´ ì •ë³´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n- í† í° ìˆ˜ê°€ ì¤„ì–´ë“¤ì–´ ì²˜ë¦¬ ë¹„ìš©ì„ 34% ì •ë„ ì ˆê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n- APIë¥¼ í˜¸ì¶œí•  ë•Œ í† í° ì‚¬ìš©ëŸ‰ì— ë”°ë¼ ë¹„ìš©ì´ ì±…ì •ë˜ëŠ” êµ¬ì¡°ì—ì„œ ìœ ë¦¬í•©ë‹ˆë‹¤.\n\níŠ¹íˆ ë¬¸ì„œ ìš”ì•½ì´ë‚˜ ê²€ìƒ‰ ì¦ê°• ìƒì„±(RAG) ë“± ê¸´ ê¸€ì„ ë‹¤ë£¨ëŠ” ê¸°ì—… í™˜ê²½ì—ì„œ, í† í° íš¨ìœ¨ì„±ì€ ìš´ì˜ ë¹„ìš©ì„ í¬ê²Œ ì ˆê°í•˜ëŠ” ë° ê¸°ì—¬í•©ë‹ˆë‹¤.\n\n### í•œêµ­ì–´ ì´í•´ì™€ ìƒì„± ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” í•™ìŠµ ë°ì´í„° êµ¬ì„±\n\nA.X 4.0ì— ì‚¬ìš©ëœ í•™ìŠµ ë°ì´í„°ëŠ” ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì§•ì„ ê°–ìŠµë‹ˆë‹¤.\n\n- ê³ í’ˆì§ˆì˜ í•œêµ­ì–´ ìë£Œ: ì›¹ì—ì„œ ì¶”ì¶œí•œ ê³ í’ˆì§ˆ ë°ì´í„°, ì „ë¬¸ ì„œì , í•©ì„± ë°ì´í„°ë¥¼ í¬í•¨í•œ ëŒ€ê·œëª¨ ê³ í’ˆì§ˆ ë°ì´í„°ì…‹ì„ í™œìš©í–ˆìŠµë‹ˆë‹¤.\n- ì²´ê³„ì ì¸ ë°ì´í„° ë¶„ë¥˜: ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ê· í˜•ìˆê²Œ ë†’ì€ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ë„ë¡ ì£¼ì œë³„ë¡œ ë¶„ë¥˜ëœ ë°ì´í„°ì…‹ì„ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤.\n- ê· í˜• ì¡íŒ ì–¸ì–´ ë¶„í¬: í•œêµ­ì–´ 42%, ì˜ì–´ 51%, ê¸°íƒ€ ì–¸ì–´ ë° ì½”ë“œ 7%ë¡œ êµ¬ì„±í•´ ì–¸ì–´ ê°„ ê· í˜•ì„ ìœ ì§€í–ˆìŠµë‹ˆë‹¤.\n\nì´ëŸ¬í•œ ë°ì´í„° êµ¬ì„±ì€ ëª¨ë¸ì´ í•œêµ­ì–´ì˜ ë‹¤ì–‘í•œ í‘œí˜„ê³¼ ë¯¸ë¬˜í•œ ë¬¸ë§¥ê¹Œì§€ ê¹Šì´ ì´í•´í•˜ë„ë¡ ë•ìŠµë‹ˆë‹¤.\n\"\"\"\n\n```\n\n\n\n## License\n\nThe `A.X 4.0 VL Light` model is licensed under `Apache License 2.0`.\n\n## Citation\n\n```\n@article{SKTAdotX4VLLight,\n  title={A.X 4.0 VL Light},\n  author={SKT AI Model Lab},\n  year={2025},\n  url={https://huggingface.co/skt/A.X-4.0-VL-Light}\n}\n```\n\n## Contact\n\n- Business & Partnership Contact: [a.x@sk.com](a.x@sk.com)\n",
    "config": "{\n  \"architectures\": [\n    \"AX4VLForConditionalGeneration\"\n  ],\n  \"auto_map\": {\n    \"AutoConfig\": \"configuration_ax4vl.AX4VLConfig\",\n    \"AutoModelForCausalLM\": \"modeling_ax4vl.AX4VLForConditionalGeneration\",\n    \"AutoProcessor\": \"processing_ax4vl.AX4VLProcessor\"\n  },\n  \"downsample_ratio\": 0.5,\n  \"dynamic_image_size\": true,\n  \"force_image_size\": 384,\n  \"image_token_index\": 22,\n  \"llm_config\": {\n    \"_attn_implementation_autoset\": false,\n    \"add_cross_attention\": false,\n    \"architectures\": [\n      \"Qwen2ForCausalLM\"\n    ],\n    \"attention_dropout\": 0.0,\n    \"attn_implementation\": \"flash_attention_2\",\n    \"bad_words_ids\": null,\n    \"begin_suppress_tokens\": null,\n    \"bos_token_id\": 0,\n    \"chunk_size_feed_forward\": 0,\n    \"cross_attention_hidden_size\": null,\n    \"decoder_start_token_id\": null,\n    \"diversity_penalty\": 0.0,\n    \"do_sample\": false,\n    \"early_stopping\": false,\n    \"encoder_no_repeat_ngram_size\": 0,\n    \"eos_token_id\": 0,\n    \"exponential_decay_length_penalty\": null,\n    \"finetuning_task\": null,\n    \"forced_bos_token_id\": null,\n    \"forced_eos_token_id\": null,\n    \"hidden_act\": \"silu\",\n    \"hidden_size\": 3584,\n    \"id2label\": {\n      \"0\": \"LABEL_0\",\n      \"1\": \"LABEL_1\"\n    },\n    \"initializer_range\": 0.02,\n    \"intermediate_size\": 18944,\n    \"is_decoder\": false,\n    \"is_encoder_decoder\": false,\n    \"label2id\": {\n      \"LABEL_0\": 0,\n      \"LABEL_1\": 1\n    },\n    \"length_penalty\": 1.0,\n    \"max_length\": 20,\n    \"max_position_embeddings\": 16384,\n    \"max_window_layers\": 28,\n    \"min_length\": 0,\n    \"model_type\": \"qwen2\",\n    \"no_repeat_ngram_size\": 0,\n    \"num_attention_heads\": 28,\n    \"num_beam_groups\": 1,\n    \"num_beams\": 1,\n    \"num_hidden_layers\": 28,\n    \"num_key_value_heads\": 4,\n    \"num_return_sequences\": 1,\n    \"output_attentions\": false,\n    \"output_hidden_states\": false,\n    \"output_scores\": false,\n    \"pad_token_id\": 1,\n    \"prefix\": null,\n    \"problem_type\": null,\n    \"pruned_heads\": {},\n    \"remove_invalid_values\": false,\n    \"repetition_penalty\": 1.0,\n    \"return_dict\": true,\n    \"return_dict_in_generate\": false,\n    \"rms_norm_eps\": 1e-05,\n    \"rope_scaling\": null,\n    \"rope_theta\": 1000000,\n    \"sep_token_id\": null,\n    \"sliding_window\": null,\n    \"suppress_tokens\": null,\n    \"task_specific_params\": null,\n    \"temperature\": 1.0,\n    \"tf_legacy_loss\": false,\n    \"tie_encoder_decoder\": false,\n    \"tie_word_embeddings\": false,\n    \"tokenizer_class\": null,\n    \"top_k\": 50,\n    \"top_p\": 1.0,\n    \"torch_dtype\": \"bfloat16\",\n    \"torchscript\": false,\n    \"typical_p\": 1.0,\n    \"use_bfloat16\": false,\n    \"use_cache\": false,\n    \"use_sliding_window\": false,\n    \"vocab_size\": 102400\n  },\n  \"max_dynamic_patch\": 12,\n  \"max_num_tiles\": 12,\n  \"min_dynamic_patch\": 1,\n  \"min_num_tiles\": 1,\n  \"model_type\": \"a.x-4-vl\",\n  \"pad_token_id\": 1,\n  \"projector_config\": {\n    \"grid_size\": 12,\n    \"in_hidden_size\": 1152,\n    \"model_type\": \"ldpnetv2_projector\",\n    \"out_hidden_size\": 3584,\n    \"torch_dtype\": \"bfloat16\"\n  },\n  \"ps_version\": \"v2\",\n  \"select_layer\": -1,\n  \"template\": \"axvlm\",\n  \"text_config\": {\n    \"architectures\": [\n      \"Qwen2ForCausalLM\"\n    ],\n    \"attn_implementation\": \"flash_attention_2\",\n    \"bos_token_id\": 0,\n    \"eos_token_id\": 0,\n    \"hidden_size\": 3584,\n    \"intermediate_size\": 18944,\n    \"max_position_embeddings\": 16384,\n    \"model_type\": \"qwen2\",\n    \"num_attention_heads\": 28,\n    \"num_hidden_layers\": 28,\n    \"num_key_value_heads\": 4,\n    \"pad_token_id\": 1,\n    \"rms_norm_eps\": 1e-05,\n    \"rope_theta\": 1000000,\n    \"sliding_window\": null,\n    \"torch_dtype\": \"bfloat16\",\n    \"use_cache\": false,\n    \"vocab_size\": 102400\n  },\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"use_thumbnail\": true,\n  \"vision_config\": {\n    \"drop_path_rate\": 0.0,\n    \"hidden_size\": 1152,\n    \"image_size\": 384,\n    \"intermediate_size\": 4304,\n    \"model_type\": \"siglip_vision_model\",\n    \"num_attention_heads\": 16,\n    \"num_hidden_layers\": 27,\n    \"torch_dtype\": \"bfloat16\",\n    \"vision_use_head\": false\n  },\n  \"vision_feature_layer\": 0,\n  \"vision_feature_select_strategy\": \"full\"\n}\n",
    "generation_config": "{\n  \"_from_model_config\": true,\n  \"bos_token_id\": 0,\n  \"eos_token_id\": [\n    0,\n    27,\n    1\n  ],\n  \"pad_token_id\": 1,\n  \"transformers_version\": \"4.49.0\",\n  \"use_cache\": false\n}\n",
    "license_file": "Copyright (c) 2025 SK Telecom Co., Ltd. All rights reserved.\n\nUnless otherwise stated, all files in this repository (including modified model weights \nand tokenizer files) are distributed under the terms of the Apache License, Version 2.0 \n(the \"License\"). You may obtain a copy of the License at:\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under \nthe License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF \nANY KIND, either express or implied. See the License for the specific language governing \npermissions and limitations under the License.\n\n================================================================================\nTRADEMARK\n================================================================================\n\n\"SK Telecom\" and associated logos are trademarks of SK Telecom Co., Ltd. \nThis License does not grant permission to use these trademarks without prior \nwritten consent.\n\n================================================================================\nAPACHE LICENSE 2.0\n================================================================================\n\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n   Copyright 2024 Alibaba Cloud\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n       http://www.apache.org/licenses/LICENSE-2.0\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.",
    "py_files": {
        "configuration_ax4vl.py": "import transformers\nfrom transformers.utils import logging\nfrom transformers.models.auto import CONFIG_MAPPING, AutoConfig\nfrom transformers.configuration_utils import PretrainedConfig\n\nlogger = logging.get_logger(__name__)\n\nclass LDPConfig(PretrainedConfig):\n    model_type = \"ldpnetv2_projector\"\n\n    def __init__(\n        self,\n        in_hidden_size=1024,\n        out_hidden_size=2048,\n        grid_size=12,\n        **kwargs\n    ):\n        self.in_hidden_size = in_hidden_size\n        self.out_hidden_size = out_hidden_size\n        self.grid_size = grid_size\n\n        super().__init__(**kwargs)\n\nclass MLPProjectorConfig(PretrainedConfig):\n    model_type = \"mlp2x_projector\"\n\n    def __init__(\n        self,\n        hidden_act=\"gelu\",\n        in_hidden_size=1024,\n        out_hidden_size=2048,\n        bias: bool=True,\n        **kwargs\n    ):\n        self.hidden_act = hidden_act\n        self.in_hidden_size = in_hidden_size\n        self.out_hidden_size = out_hidden_size\n        self.bias = bias\n\n        super().__init__(**kwargs)\n\n\n\nclass AX4VLConfig(PretrainedConfig):\n    model_type = \"a.x-4-vl\"\n    sub_configs = {\n        \"text_config\": AutoConfig,\n        \"projector_config\": AutoConfig,\n        \"vision_config\": AutoConfig\n    }\n\n    def __init__(\n        self,\n        vision_config=None,\n        projector_config=None,\n        text_config=None,\n        image_token_index=102400,\n        vision_feature_select_strategy=\"full\",\n        vision_feature_layer=0,\n        tie_word_embeddings=False,\n        **kwargs,\n    ):\n        self.image_token_index = image_token_index\n\n        if vision_feature_select_strategy not in [\"default\", \"full\"]:\n            raise ValueError(\n                \"vision_feature_select_strategy should be one of 'default', 'full'.\"\n                f\"Got: {vision_feature_select_strategy}\"\n            )\n\n        self.vision_feature_select_strategy = vision_feature_select_strategy\n        self.vision_feature_layer = vision_feature_layer\n\n        if isinstance(vision_config, dict):\n            vision_config[\"model_type\"] = (\n                vision_config[\"model_type\"] if \"model_type\" in vision_config else \"siglip_vision_model\"\n            )\n            vision_config = CONFIG_MAPPING[vision_config[\"model_type\"]](**vision_config)\n        elif vision_config is None:\n            vision_config = CONFIG_MAPPING[\"siglip_vision_model\"](\n                intermediate_size=4304,\n                hidden_size=1152,\n                patch_size=16,\n                image_size=384,\n                num_hidden_layers=27,\n                num_attention_heads=16,\n                vision_use_head=False\n            )\n        self.vision_config = vision_config\n\n        if isinstance(projector_config, dict):\n            projector_config[\"model_type\"] = (\n                projector_config[\"model_type\"] if \"model_type\" in projector_config else \"mlp2x\"\n            )\n            projector_config = CONFIG_MAPPING[projector_config[\"model_type\"]](**projector_config)\n        elif projector_config is None:\n            projector_config = CONFIG_MAPPING[\"mlp2x_projector\"]()\n        self.projector_config = projector_config\n\n        if isinstance(text_config, dict):\n            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"qwen2\"\n            text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n        elif text_config is None:\n            text_config = CONFIG_MAPPING[\"qwen2\"]()\n\n        self.text_config = text_config\n\n        super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n\n\nAutoConfig.register(LDPConfig.model_type, LDPConfig)\nAutoConfig.register(MLPProjectorConfig.model_type, MLPProjectorConfig)\nAutoConfig.register(AX4VLConfig.model_type, AX4VLConfig)",
        "image_processing_ax4vl.py": "\"\"\"\nImage processor class for Megatron-LM LLaVA.\n\"\"\"\n\nimport math\nfrom typing import Dict, Iterable, List, Optional, Tuple, Union\n\nimport numpy as np\nfrom PIL import Image\nfrom .configuration_ax4vl import AX4VLConfig\n\nfrom transformers.image_utils import (\n    OPENAI_CLIP_MEAN,\n    OPENAI_CLIP_STD,\n    ChannelDimension,\n    ImageInput,\n    PILImageResampling,\n    infer_channel_dimension_format,\n    is_scaled_image,\n    is_valid_image,\n    valid_images,\n    make_list_of_images,\n    to_numpy_array,\n    validate_preprocess_arguments,\n)\nfrom transformers.image_processing_utils import BatchFeature, get_size_dict, BaseImageProcessor\nfrom transformers.image_transforms import (\n    PaddingMode,\n    pad,\n    to_channel_dimension_format,\n)\nfrom transformers.utils import TensorType, logging\nfrom transformers.models.auto import AutoImageProcessor\n\n\nlogger = logging.get_logger(__name__)\n\ndef _get_patch_output_size(image, target_resolution):\n    original_width, original_height = image.size\n    target_width, target_height = target_resolution\n\n    scale_w = target_width / original_width\n    scale_h = target_height / original_height\n\n    if scale_w < scale_h:\n        new_width = target_width\n        new_height = min(math.ceil(original_height * scale_w), target_height)\n    else:\n        new_height = target_height\n        new_width = min(math.ceil(original_width * scale_h), target_width)\n\n    return new_width, new_height\n\n# From https://github.com/OpenGVLab/InternVL/blob/c62fa4f7c850165d7386bdc48ac6bc5a6fab0864/internvl_chat/internvl/train/dataset.py#L685\n# Copyright (c) 2023 OpenGVLab.\ndef find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n    best_ratio_diff = float('inf')\n    best_ratio = (1, 1)\n    area = width * height\n    for ratio in target_ratios:\n        target_aspect_ratio = ratio[0] / ratio[1]\n        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n        if ratio_diff < best_ratio_diff:\n            best_ratio_diff = ratio_diff\n            best_ratio = ratio\n        elif ratio_diff == best_ratio_diff:\n            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n                best_ratio = ratio\n    # print(f'width: {width}, height: {height}, best_ratio: {best_ratio}')\n    return best_ratio\n\ndef _pad_for_patching(image, target_resolution, background_color=(0, 0, 0)):\n    \"\"\"\n    Pad an image to a target resolution while maintaining aspect ratio.\n    \"\"\"\n    target_width, target_height = target_resolution\n    new_width, new_height = _get_patch_output_size(image, target_resolution)\n\n    paste_x = (target_width - new_width) // 2\n    paste_y = (target_height - new_height) // 2\n\n    padded_image = Image.new(image.mode, target_resolution, background_color)\n    padded_image.paste(image, (paste_x, paste_y))\n    return padded_image\n\ndef _resize_for_patching(image, target_resolution):\n    new_size = _get_patch_output_size(image, target_resolution)\n\n    # Resize the image\n    resized_image = image.resize(new_size)\n\n    return resized_image\n\ndef get_target_ratios(image_size, min_num=1, max_num=6, tile_size=384):\n    orig_width, orig_height = image_size\n    aspect_ratio = orig_width / orig_height\n\n    target_ratios = set(\n        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n        i * j <= max_num and i * j >= min_num)\n    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n\n    return find_closest_aspect_ratio(\n        aspect_ratio, target_ratios, orig_width, orig_height, tile_size\n    )\n\n# From https://github.com/OpenGVLab/InternVL/blob/c62fa4f7c850165d7386bdc48ac6bc5a6fab0864/internvl_chat/internvl/train/dataset.py#L702\n# Copyright (c) 2023 OpenGVLab.\ndef dynamic_preprocess(image, min_num=1, max_num=6, image_size=448, use_thumbnail=False, padding=False):\n    # find the closest aspect ratio to the target\n    target_aspect_ratio = get_target_ratios(image.size, min_num=min_num, max_num=max_num, tile_size=image_size)\n\n    # calculate the target width and height\n    target_width = image_size * target_aspect_ratio[0]\n    target_height = image_size * target_aspect_ratio[1]\n    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n\n    # resize the image\n    if padding: # LLaVA-Next tiling strategy\n        resized_img = _resize_for_patching(image, (target_width, target_height))\n        resized_img = _pad_for_patching(resized_img, (target_width, target_height))\n    else:   # InternVL tiling strategy\n        resized_img = image.resize((target_width, target_height))\n    processed_images = []\n    for i in range(blocks):\n        box = (\n            (i % (target_width // image_size)) * image_size,\n            (i // (target_width // image_size)) * image_size,\n            ((i % (target_width // image_size)) + 1) * image_size,\n            ((i // (target_width // image_size)) + 1) * image_size\n        )\n        # split the image\n        split_img = resized_img.crop(box)\n        processed_images.append(split_img)\n    assert len(processed_images) == blocks\n    if use_thumbnail and len(processed_images) != 1:\n        thumbnail_img = image.resize((image_size, image_size))\n        processed_images.append(thumbnail_img)\n    return processed_images\n\nclass AX4VLImageProcessor(BaseImageProcessor):\n\n    model_input_names = [\"pixel_values\"]\n\n    def __init__(\n        self,\n        do_resize: bool = True,\n        size: Dict[str, int] = None,\n        resample: PILImageResampling = PILImageResampling.BICUBIC,\n        do_rescale: bool = True,\n        rescale_factor: Union[int, float] = 1 / 255,\n        do_normalize: bool = True,\n        image_mean: Optional[Union[float, List[float]]] = None,\n        image_std: Optional[Union[float, List[float]]] = None,\n        do_pad: Optional[bool] = True,\n        do_tile_pad: Optional[bool] = True,\n        do_convert_rgb: bool = True,\n        use_thumbnail: bool = True,\n        min_num_tiles: int = 1,\n        max_num_tiles: int = 6,\n        **kwargs,\n    ) -> None:\n        super().__init__(**kwargs)\n        size = dict(size) if size is not None else {\"shortest_edge\": 224}\n        size = get_size_dict(size, default_to_square=False)\n\n        self.do_resize = do_resize\n        self.size = size\n        self.resample = resample\n        self.do_rescale = do_rescale\n        self.rescale_factor = rescale_factor\n        self.do_normalize = do_normalize\n        self.image_mean = image_mean if image_mean is not None else OPENAI_CLIP_MEAN\n        self.image_std = image_std if image_std is not None else OPENAI_CLIP_STD\n        self.do_pad = do_pad\n        self.do_tile_pad = do_tile_pad\n        self.do_convert_rgb = do_convert_rgb\n        self.use_thumbnail = use_thumbnail\n        self.min_num_tiles = min_num_tiles\n        self.max_num_tiles = max_num_tiles\n\n    def pad(\n        self,\n        image: np.ndarray,\n        padding: Union[int, Tuple[int, int], Iterable[Tuple[int, int]]],\n        mode: PaddingMode = PaddingMode.CONSTANT,\n        constant_values: Union[float, Iterable[float]] = 0.0,\n        data_format: Optional[Union[str, ChannelDimension]] = None,\n        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n    ) -> np.ndarray:\n        \"\"\"\n        Pads the `image` with the specified `padding` and `mode`. Padding can be in the (`height`, `width`)\n        dimension of in the (`num_patches`) dimension. In the second case an iterable if tuples is expected\n        as input.\n\n        Args:\n            image (`np.ndarray`):\n                The image to pad.\n            padding (`int` or `Tuple[int, int]` or `Iterable[Tuple[int, int]]`):\n                Padding to apply to the edges of the height, width axes. Can be one of three formats:\n                - `((before_height, after_height), (before_width, after_width))` unique pad widths for each axis.\n                - `((before, after),)` yields same before and after pad for height and width.\n                - `(pad,)` or int is a shortcut for before = after = pad width for all axes.\n            mode (`PaddingMode`):\n                The padding mode to use. Can be one of:\n                    - `\"constant\"`: pads with a constant value.\n                    - `\"reflect\"`: pads with the reflection of the vector mirrored on the first and last values of the\n                    vector along each axis.\n                    - `\"replicate\"`: pads with the replication of the last value on the edge of the array along each axis.\n                    - `\"symmetric\"`: pads with the reflection of the vector mirrored along the edge of the array.\n            constant_values (`float` or `Iterable[float]`, *optional*):\n                The value to use for the padding if `mode` is `\"constant\"`.\n            data_format (`str` or `ChannelDimension`, *optional*):\n                The channel dimension format for the output image. Can be one of:\n                    - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n                    - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n                If unset, will use same as the input image.\n            input_data_format (`str` or `ChannelDimension`, *optional*):\n                The channel dimension format for the input image. Can be one of:\n                    - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n                    - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n                If unset, will use the inferred format of the input image.\n\n        Returns:\n            `np.ndarray`: The padded image.\n\n        \"\"\"\n\n        # call the general `pad` if padding on `height/width`, otherwise it's the `num_patched` dim\n        if isinstance(padding, int) or len(padding) != 4:\n            return pad(image, padding, mode, constant_values, data_format, input_data_format)\n\n        if input_data_format is None:\n            input_data_format = infer_channel_dimension_format(image)\n        if mode == PaddingMode.CONSTANT:\n            image = np.pad(image, padding, mode=\"constant\", constant_values=constant_values)\n        elif mode == PaddingMode.REFLECT:\n            image = np.pad(image, padding, mode=\"reflect\")\n        elif mode == PaddingMode.REPLICATE:\n            image = np.pad(image, padding, mode=\"edge\")\n        elif mode == PaddingMode.SYMMETRIC:\n            image = np.pad(image, padding, mode=\"symmetric\")\n        else:\n            raise ValueError(f\"Invalid padding mode: {mode}\")\n        image = (\n            to_channel_dimension_format(image, data_format, input_data_format) if data_format is not None else image\n        )\n        return image\n\n    def _pad_for_batching(\n        self,\n        pixel_values: List[np.ndarray],\n        data_format: Optional[Union[str, ChannelDimension]] = None,\n        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n    ):\n        \"\"\"\n        Pads images on the `num_of_patches` dimension with zeros to form a batch of same number of patches.\n\n        Args:\n            pixel_values (`List[np.ndarray]`):\n                An array of pixel values of each images of shape (`batch_size`, `num_patches`, `image_in_3D`)\n            data_format (`str` or `ChannelDimension`, *optional*):\n                The channel dimension format for the output image. Can be one of:\n                    - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n                    - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n                If unset, will use same as the input image.\n            input_data_format (`str` or `ChannelDimension`, *optional*):\n                The channel dimension format for the input image. Can be one of:\n                    - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n                    - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n                If unset, will use the inferred format of the input image.\n\n        Returns:\n            List[`np.ndarray`]: The padded images.\n        \"\"\"\n        max_patch = max(len(x) for x in pixel_values)\n        pixel_values = [\n            self.pad(\n                image,\n                padding=((0, max_patch - image.shape[0]), (0, 0), (0, 0), (0, 0)),\n                data_format=data_format,\n                input_data_format=input_data_format,\n            )\n            for image in pixel_values\n        ]\n\n        return pixel_values\n\n    def _preprocess(\n        self,\n        images: ImageInput,\n        do_resize: bool = None,\n        size: Dict[str, int] = None,\n        resample: PILImageResampling = None,\n        do_rescale: bool = None,\n        rescale_factor: float = None,\n        do_normalize: bool = None,\n        image_mean: Optional[Union[float, List[float]]] = None,\n        image_std: Optional[Union[float, List[float]]] = None,\n        do_convert_rgb: bool = None,\n        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n    ):\n        images = make_list_of_images(images)\n\n        all_images = []\n        for image in images:\n            if do_resize:\n                image = image.resize((size[\"shortest_edge\"], size[\"shortest_edge\"]), resample)\n\n            image = to_numpy_array(image)\n\n            if input_data_format is None:\n                # We assume that all images have the same channel dimension format.\n                input_data_format = infer_channel_dimension_format(image)\n\n            if is_scaled_image(image) and do_rescale:\n                logger.warning_once(\n                    \"It looks like you are trying to rescale already rescaled images. If the input\"\n                    \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n                )\n            if do_rescale:\n                image = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n\n            if do_normalize:\n                image = self.normalize(\n                    image=image, mean=image_mean, std=image_std, input_data_format=input_data_format\n                )\n\n            all_images.append(image)\n\n        images = [\n            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n            for image in all_images\n        ]\n\n        return images\n\n    def preprocess(\n        self,\n        images: ImageInput,\n        do_resize: bool = None,\n        size: Dict[str, int] = None,\n        resample: PILImageResampling = None,\n        do_rescale: bool = None,\n        rescale_factor: float = None,\n        do_normalize: bool = None,\n        image_mean: Optional[Union[float, List[float]]] = None,\n        image_std: Optional[Union[float, List[float]]] = None,\n        do_pad: Optional[bool] = None,\n        do_convert_rgb: bool = None,\n        return_tensors: Optional[Union[str, TensorType]] = None,\n        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n    ):\n        \"\"\"\n        Args:\n            images (`ImageInput`):\n                Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255.\n            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n                Whether to resize the image.\n            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n                Size of the image after resizing. Shortest edge of the image is resized to size[\"shortest_edge\"], with\n                the longest edge resized to keep the input aspect ratio.\n            resample (`int`, *optional*, defaults to `self.resample`):\n                Resampling filter to use if resizing the image. This can be one of the enum `PILImageResampling`. Only\n                has an effect if `do_resize` is set to `True`.\n            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n                Whether to normalize the image.\n            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n                Image mean to use for normalization. Only has an effect if `do_normalize` is set to `True`.\n            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n                Image standard deviation to use for normalization. Only has an effect if `do_normalize` is set to\n                `True`.\n            do_pad (`bool`, *optional*, defaults to `self.do_pad`):\n                Whether to pad the image. If `True`, will pad the patch dimension of the images in the batch to the largest\n                number of patches in the batch. Padding will be applied to the bottom and right with zeros.\n            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n                Whether to convert the image to RGB.\n            return_tensors (`str` or `TensorType`, *optional*):\n                The type of tensors to return. Can be one of:\n                - Unset: Return a list of `np.ndarray`.\n                - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of type `tf.Tensor`.\n                - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n                - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n                - `TensorType.JAX` or `'jax'`: Return a batch of type `jax.numpy.ndarray`.\n            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n                The channel dimension format for the output image. Can be one of:\n                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n                - Unset: Use the channel dimension format of the input image.\n            input_data_format (`ChannelDimension` or `str`, *optional*):\n                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n                from the input image. Can be one of:\n                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n\n        \"\"\"\n        do_resize = do_resize if do_resize is not None else self.do_resize\n        size = size if size is not None else self.size\n        size = get_size_dict(size, param_name=\"size\", default_to_square=False)\n        resample = resample if resample is not None else self.resample\n        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n        image_mean = image_mean if image_mean is not None else self.image_mean\n        image_std = image_std if image_std is not None else self.image_std\n        do_pad = do_pad if do_pad is not None else self.do_pad\n        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n\n        images = make_batched_images(images)\n\n        if not valid_images(images):\n            raise ValueError(\n                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n            )\n\n        validate_preprocess_arguments(\n            do_rescale=do_rescale,\n            rescale_factor=rescale_factor,\n            do_normalize=do_normalize,\n            image_mean=image_mean,\n            image_std=image_std,\n            do_resize=do_resize,\n            size=size,\n            resample=resample,\n        )\n\n        new_images, num_tiles = [], []\n        image_sizes = [image.size for image in images]\n        for image in images:\n            if do_convert_rgb and image.mode != \"RGB\":\n                image = image.convert(\"RGB\")\n\n            image_patches = dynamic_preprocess(\n                image,\n                min_num=self.min_num_tiles,\n                max_num=self.max_num_tiles,\n                image_size=self.size[\"shortest_edge\"],\n                use_thumbnail=self.use_thumbnail,\n                padding=self.do_tile_pad\n            )\n\n            # preprocess patches\n            pixel_values = self._preprocess(\n                image_patches,\n                do_resize=do_resize,\n                size=size,\n                resample=resample,\n                do_rescale=do_rescale,\n                rescale_factor=rescale_factor,\n                do_normalize=do_normalize,\n                image_mean=image_mean,\n                image_std=image_std,\n                data_format=data_format,\n                input_data_format=input_data_format\n            )\n            pixel_values = np.array(pixel_values)\n            new_images.append(pixel_values)\n            num_tiles.append(len(image_patches))\n\n        if do_pad:\n            processed_images = self._pad_for_batching(new_images)\n        else:\n            processed_images = np.concatenate(new_images)\n\n        return BatchFeature(\n            data={\"pixel_values\": processed_images, \"image_sizes\": image_sizes, \"num_tiles\": num_tiles},\n            tensor_type=return_tensors\n        )\n\n\ndef make_batched_images(images) -> List[List[ImageInput]]:\n    \"\"\"\n    Accepts images in list or nested list format, and makes a list of images for preprocessing.\n\n    Args:\n        images (`Union[List[List[ImageInput]], List[ImageInput], ImageInput]`):\n            The input image.\n\n    Returns:\n        list: A list of images.\n    \"\"\"\n    if isinstance(images, (list, tuple)) and isinstance(images[0], (list, tuple)) and is_valid_image(images[0][0]):\n        return [img for img_list in images for img in img_list]\n\n    elif isinstance(images, (list, tuple)) and is_valid_image(images[0]):\n        return images\n\n    elif is_valid_image(images):\n        return [images]\n\n    raise ValueError(f\"Could not make batched video from {images}\")\n\nAutoImageProcessor.register(AX4VLConfig, AX4VLImageProcessor)\n",
        "modeling_ax4vl.py": "\"\"\"\nbase code: LLaVA-Next (transformers==4.49.0)\n\"\"\"\nfrom typing import List, Optional, Tuple, Union\nimport math\n\nimport torch\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom .configuration_ax4vl import LDPConfig, MLPProjectorConfig, AX4VLConfig\n\nfrom transformers.activations import ACT2FN\nfrom transformers.generation import GenerationMixin\nfrom transformers.models.auto import AutoModel, AutoModelForCausalLM\nfrom transformers.utils import (\n    is_torchdynamo_compiling,\n    logging,\n)\nfrom transformers.configuration_utils import PretrainedConfig\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.modeling_outputs import ModelOutput\nfrom dataclasses import dataclass\n\n\n\nlogger = logging.get_logger(__name__)\n\ndef build_projector(config):\n    if config.model_type == \"ldpnetv2_projector\":\n        return LDPProjector(config)\n    else:\n        raise ValueError(f\"Unknown projector type: {config.model_type}\")\n\n@dataclass\nclass AX4CausalLMOutputWithPast(ModelOutput):\n    loss: Optional[torch.FloatTensor] = None\n    logits: torch.FloatTensor = None\n    past_key_values: Optional[List[torch.FloatTensor]] = None\n    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n    attentions: Optional[Tuple[torch.FloatTensor]] = None\n    image_hidden_states: Optional[torch.FloatTensor] = None\n\n\nclass BaseAXPretrainedModel(PreTrainedModel):\n    config_class = PretrainedConfig\n    base_model_prefix = \"model\"\n    supports_gradient_checkpointing = True\n    _no_split_modules = [\"AXVisionAttention\"]\n    _skip_keys_device_placement = \"past_key_values\"\n    _supports_cache_class = True\n    _supports_flash_attn_2 = True\n    _supports_sdpa = True\n    _supports_quantized_cache = True\n    _supports_static_cache = True\n\n    def __init__(self, config: PretrainedConfig):\n        super().__init__(config)\n\n    def _init_weights(self, module):\n        # important: this ported version of LlavaNext isn't meant for training from scratch - only\n        # inference and fine-tuning - so the proper init weights code has been removed - the original codebase\n        # https://github.com/haotian-liu/LLaVA/tree/main/llava_next should serve for that purpose\n        std = (\n            self.config.initializer_range\n            if hasattr(self.config, \"initializer_range\")\n            else self.config.text_config.initializer_range\n        )\n\n        if hasattr(module, \"class_embedding\"):\n            module.class_embedding.data.normal_(mean=0.0, std=std)\n\n        if isinstance(module, (nn.Linear, nn.Conv2d)):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n\n\n\nclass AX4VLForConditionalGeneration(BaseAXPretrainedModel, GenerationMixin):\n    config_class = AX4VLConfig\n\n    def __init__(self, config: AX4VLConfig):\n        super().__init__(config)\n        self.vision_tower = AutoModel.from_config(config.vision_config)\n\n        self.multi_modal_projector = build_projector(config.projector_config)\n        self.vocab_size = config.text_config.vocab_size\n        self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n        if self.language_model._tied_weights_keys is not None:\n            self._tied_weights_keys = [f\"language_model.{k}\" for k in self.language_model._tied_weights_keys]\n\n        self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.language_model.get_input_embeddings()\n\n    def set_input_embeddings(self, value):\n        self.language_model.set_input_embeddings(value)\n\n    def get_output_embeddings(self):\n        return self.language_model.get_output_embeddings()\n\n    def set_output_embeddings(self, new_embeddings):\n        self.language_model.set_output_embeddings(new_embeddings)\n\n    def set_decoder(self, decoder):\n        self.language_model.set_decoder(decoder)\n\n    def get_decoder(self):\n        return self.language_model.get_decoder()\n\n    def get_image_features(\n        self,\n        pixel_values: torch.FloatTensor,\n        vision_feature_layer: Union[int, List[int]],\n        vision_feature_select_strategy: str,\n    ):\n        if pixel_values.dim() != 4:\n            # otherwise has to be stacked from list of (num_patches, num_channels, height, width)\n            raise ValueError(f\"pixel_values of shape {pixel_values.shape}, expect to be of 4 or 5 dimensions\")\n\n        image_outputs = self.vision_tower(pixel_values, output_hidden_states=True)\n        # If we have one vision feature layer, return the corresponding hidden states,\n        # otherwise, select the hidden states of each feature layer and concatenate them\n        if isinstance(vision_feature_layer, int):\n            if vision_feature_layer == 0:\n                selected_image_feature = image_outputs.last_hidden_state\n            else:\n                selected_image_feature = image_outputs.hidden_states[vision_feature_layer]\n        else:\n            hs_pool = [image_features.hidden_states[layer_idx] for layer_idx in vision_feature_layer]\n            selected_image_feature = torch.cat(hs_pool, dim=-1)\n\n        if vision_feature_select_strategy == \"default\":\n            selected_image_feature = selected_image_feature[:, 1:]\n        elif vision_feature_select_strategy == \"full\":\n            selected_image_feature = selected_image_feature\n\n        image_features = self.multi_modal_projector(selected_image_feature)\n        return image_features\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        pixel_values: torch.FloatTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n        vision_feature_select_strategy: Optional[str] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n        logits_to_keep: Union[int, torch.Tensor] = 0,\n        **lm_kwargs,\n    ) -> Union[Tuple, AX4CausalLMOutputWithPast]:\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        vision_feature_layer = (\n            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n        )\n        vision_feature_select_strategy = (\n            vision_feature_select_strategy\n            if vision_feature_select_strategy is not None\n            else self.config.vision_feature_select_strategy\n        )\n\n        if (input_ids is None) ^ (inputs_embeds is not None):\n            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n\n        if pixel_values is not None and inputs_embeds is not None:\n            raise ValueError(\n                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n            )\n\n        if inputs_embeds is None:\n            inputs_embeds = self.get_input_embeddings()(input_ids)\n\n        if pixel_values is not None and pixel_values.size(0) > 0:\n            image_features = self.get_image_features(\n                pixel_values,\n                vision_feature_layer=vision_feature_layer,\n                vision_feature_select_strategy=vision_feature_select_strategy,\n            )\n\n            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n                n_image_tokens = (input_ids == self.config.image_token_index).sum()\n                n_image_features = image_features.shape[0]\n                raise ValueError(\n                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n                )\n            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n\n        outputs = self.language_model(\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            cache_position=cache_position,\n            logits_to_keep=logits_to_keep,\n            **lm_kwargs,\n        )\n\n        logits = outputs[0]\n\n        loss = None\n        if labels is not None:\n            # Shift so that tokens < n predict n\n            if attention_mask is not None:\n                # we use the input attention mask to shift the logits and labels, because it is 2D.\n                # we also crop attn mask in case it is longer, which happens in PrefixTuning with peft\n                shift_attention_mask = attention_mask[:, -(logits.shape[1] - 1) :].to(logits.device)\n                shift_logits = logits[..., :-1, :][shift_attention_mask.to(logits.device) != 0].contiguous()\n                shift_labels = labels[..., 1:][shift_attention_mask.to(labels.device) != 0].contiguous()\n            else:\n                shift_logits = logits[..., :-1, :].contiguous()\n                shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(\n                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1).to(shift_logits.device)\n            )\n\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return (loss,) + output if loss is not None else output\n\n        return AX4CausalLMOutputWithPast(\n            loss=loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n            image_hidden_states=image_features if pixel_values is not None else None,\n        )\n\n    def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        past_key_values=None,\n        inputs_embeds=None,\n        pixel_values=None,\n        image_sizes=None,\n        attention_mask=None,\n        cache_position=None,\n        logits_to_keep=None,\n        **kwargs,\n    ):\n        # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n\n        model_inputs = self.language_model.prepare_inputs_for_generation(\n            input_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            attention_mask=attention_mask,\n            cache_position=cache_position,\n            logits_to_keep=logits_to_keep,\n            **kwargs,\n        )\n\n        # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n        # Otherwise we need pixel values to be passed to model\n        if cache_position[0] == 0:\n            model_inputs[\"pixel_values\"] = pixel_values\n            model_inputs[\"image_sizes\"] = image_sizes\n\n        return model_inputs\n\n\n\n\n\nclass FeatureIRLayer(nn.Module):\n    def __init__(self, in_dim: int, out_dim: int) -> None:\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.Linear(in_dim, out_dim), nn.GELU(), nn.Linear(out_dim, out_dim)\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.mlp(x)\n\n\nclass TokenDownLayer(nn.Module):\n    def __init__(self, shape) -> None:\n        super().__init__()\n        self.dwn = nn.Sequential(\n            nn.AdaptiveAvgPool2d(shape)\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        b, num_tokens, c = x.shape\n        h = int(math.sqrt(num_tokens))\n        assert h * h == num_tokens\n        x = x.permute(0, 2, 1).reshape(b, -1, h, h)\n        x = self.dwn(x)\n        x = x.flatten(2).transpose(1, 2)\n        return x\n    \n\nclass PosInjectLayer(nn.Module):\n    # https://github.com/Meituan-AutoML/Twins/blob/main/gvt.py\n    def __init__(\n            self,\n            in_dim: int,\n            out_dim: int,\n            stride: int = 1,\n            padding: int = 1,\n            shape = None) -> None:\n        super().__init__()\n        self.peg = nn.Sequential(\n            nn.Conv2d(in_dim, out_dim, 3, stride, padding, bias=True, groups=out_dim)\n        )\n        self.pool = None\n        if shape is not None:\n            self.pool = nn.Sequential(\n                nn.AdaptiveAvgPool2d(shape)\n            )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        b, num_tokens, c = x.shape\n        h = int(math.sqrt(num_tokens))\n        assert h * h == num_tokens\n        cnn_feat = x.transpose(1, 2).view(b, c, h, h)\n        if self.pool is not None:\n            x = self.peg(cnn_feat) + self.pool(cnn_feat)\n        else:\n            x = self.peg(cnn_feat) + cnn_feat\n        x = x.flatten(2).transpose(1, 2)\n        return x\n\nclass LDPProjector(PreTrainedModel):\n    config_class = LDPConfig\n    _no_split_modules = []\n\n    def __init__(self, config):\n        super().__init__(config)\n        inc, ouc = config.in_hidden_size, config.out_hidden_size\n        grid = config.grid_size\n        self.mlp = FeatureIRLayer(inc, ouc)\n        self.dwn = TokenDownLayer((grid, grid))\n        self.peg = PosInjectLayer(ouc, ouc, stride=1)\n\n    def forward(self, x):\n        x = self.mlp(x)\n        x = self.dwn(x)\n        x = self.peg(x)\n        return x\n\nclass MLPProjector(PreTrainedModel):\n    config_class = MLPProjectorConfig\n    _no_split_modules = []\n\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.linear_1 = nn.Linear(config.in_hidden_size, config.out_hidden_size, bias=config.bias)\n        self.act = ACT2FN[config.hidden_act]\n        self.linear_2 = nn.Linear(config.out_hidden_size, config.out_hidden_size, bias=config.bias)\n\n    def forward(self, image_features):\n        hidden_states = self.linear_1(image_features)\n        hidden_states = self.act(hidden_states)\n        hidden_states = self.linear_2(hidden_states)\n        return hidden_states\n\n",
        "processing_ax4vl.py": "from typing import List, Union\nfrom .configuration_ax4vl import AX4VLConfig\nfrom transformers.models.auto import AutoProcessor\nfrom transformers.feature_extraction_utils import BatchFeature\nfrom transformers.image_utils import ImageInput\nfrom transformers.tokenization_utils_base import PreTokenizedInput, TextInput\nfrom transformers.processing_utils import ProcessingKwargs, ProcessorMixin, _validate_images_text_input_order\n\n\n\nclass BaseAXProcessor(ProcessorMixin):\n    attributes = [\"image_processor\", \"tokenizer\"]\n    image_processor_class = \"AutoImageProcessor\"\n    tokenizer_class = \"AutoTokenizer\"\n\n\nclass AX4VLProcessorKwargs(ProcessingKwargs, total=False):\n    _defaults = {\n        \"text_kwargs\": {\n            \"padding\": False,\n        },\n        \"images_kwargs\": {\n            \"do_pad\": False,\n        },\n    }\n\n\nclass AX4VLProcessor(BaseAXProcessor):\n    valid_kwargs = [\n        \"chat_template\",\n        \"patch_size\",\n        \"num_tokens_per_tile\",\n        \"image_token\",\n    ]\n\n    def __init__(\n        self,\n        image_processor=None,\n        tokenizer=None,\n        patch_size=16,\n        num_tokens_per_tile=144,\n        image_token=\"<image>\",  # set the default and let users change if they have peculiar special tokens in rare cases\n        chat_template=None,\n        **kwargs\n    ):\n        self.patch_size = patch_size\n        self.num_tokens_per_tile = num_tokens_per_tile\n        self.image_token = tokenizer.image_token if hasattr(tokenizer, \"image_token\") else image_token\n        super().__init__(image_processor, tokenizer, chat_template=chat_template)\n\n    def __call__(\n        self,\n        images: ImageInput = None,\n        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n        conversations: List = None,\n        **kwargs\n    ) -> BatchFeature:\n        if images is None and conversations is None and text is None:\n            raise ValueError(\"You have to specify at least images, text or conversation.\")\n\n        if not text and conversations is not None:\n            if isinstance(conversations[0], dict):\n                conversations = [conversations]\n            text = [self.apply_chat_template(conv, **kwargs) for conv in conversations]\n\n        images, text = _validate_images_text_input_order(images, text)\n        \n        output_kwargs = self._merge_kwargs(\n            AX4VLProcessorKwargs,\n            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n            **kwargs,\n        )\n\n        if images is not None:\n            image_inputs = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n        else:\n            image_inputs = {}\n\n        prompt_strings = text\n        if image_inputs:\n            num_tiles = iter(image_inputs[\"num_tiles\"])\n            prompt_strings = []\n            for sample in text:\n                while self.image_token in sample:\n                    num_tile = next(num_tiles)\n                    num_image_tokens = num_tile * self.num_tokens_per_tile\n                    sample = sample.replace(self.image_token, \"<placeholder>\" * num_image_tokens, 1)\n                prompt_strings.append(sample)\n            prompt_strings = [sample.replace(\"<placeholder>\", self.image_token) for sample in prompt_strings]\n\n        text_inputs = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"])\n        \n        if \"num_tiles\" in image_inputs:\n            del image_inputs[\"num_tiles\"]\n        return BatchFeature(data={**text_inputs, **image_inputs})\n\n    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.batch_decode with CLIP->Llama\n    def batch_decode(self, *args, **kwargs):\n        \"\"\"\n        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n        refer to the docstring of this method for more information.\n        \"\"\"\n        return self.tokenizer.batch_decode(*args, **kwargs)\n\n    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.decode with CLIP->Llama\n    def decode(self, *args, **kwargs):\n        \"\"\"\n        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n        the docstring of this method for more information.\n        \"\"\"\n        return self.tokenizer.decode(*args, **kwargs)\n\n    @property\n    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.model_input_names\n    def model_input_names(self):\n        tokenizer_input_names = self.tokenizer.model_input_names\n        image_processor_input_names = self.image_processor.model_input_names\n        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n\n\nAutoProcessor.register(AX4VLConfig, AX4VLProcessor)\n"
    }
}