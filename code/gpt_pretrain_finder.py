#!/usr/bin/env python
# gpt_pretrain_finder.py
# ----------------------------------------------------------
# â–¶ ì‚¬ìš©ë²•
#     python gpt_pretrain_finder.py <org/model>
# ----------------------------------------------------------

import sys, os, json, requests, textwrap
from pathlib import Path
from dotenv import load_dotenv
from openai import OpenAI

# â”€â”€ í™˜ê²½ ë³€ìˆ˜ì—ì„œ OPENAI_API_KEY ë¶ˆëŸ¬ì˜¤ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()
cli = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# â”€â”€ HF ì¹´ë“œ + README í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _get_hf_context(hf_id: str, max_len: int = 12000) -> str:
    try:
        card = requests.get(
            f"https://huggingface.co/api/models/{hf_id}?full=true"
        ).json().get("cardData", {}) or {}
        txt = (card.get("content") or "")[:max_len]

        # README raw ì¶”ê°€
        for br in ["main", "master"]:
            r = requests.get(f"https://huggingface.co/{hf_id}/raw/{br}/README.md")
            if r.status_code == 200:
                txt += "\n\n" + r.text[:max_len]
                break
        return txt
    except Exception as e:
        print("âš ï¸ HF ì¹´ë“œ ë¡œë”© ì‹¤íŒ¨:", e)
        return ""

# â”€â”€ GPT-4 í”„ë¡¬í”„íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PROMPT_TMPL = textwrap.dedent("""
    ë‹¹ì‹ ì€ AI ëª¨ë¸ ì •ë³´ë¥¼ ë¶„ì„í•´ â€˜í”„ë¦¬íŠ¸ë ˆì¸(ì›) ëª¨ë¸â€™ì„ ì°¾ì•„ë‚´ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

    â€¢ ì…ë ¥ìœ¼ë¡œ ì œê³µëœ Hugging Face ëª¨ë¸ **{hf_id}** ëŠ” íŒŒì¸íŠœë‹(í˜¹ì€ ì²´í¬í¬ì¸íŠ¸) ëª¨ë¸ì¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
    â€¢ ì•„ë˜ì— ì œê³µë˜ëŠ” Hugging Face ì¹´ë“œ / README ë‚´ìš©ì„ ë¶„ì„í•´,
      â€œì´ ëª¨ë¸ì´ ì–´ë–¤ í”„ë¦¬íŠ¸ë ˆì¸ ëª¨ë¸(checkpoint)ì—ì„œ íŒŒìƒë˜ì—ˆëŠ”ì§€â€ ì¶”ì •í•˜ì„¸ìš”.
    â€¢ ë‹µë³€ í˜•ì‹ì€ **JSON í•œ ì¤„**ë¡œë§Œ, ì˜ˆì‹œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
        {{ "pretrain_model": "bigscience/bloom-560m" }}
      (ëª¨ë¸ IDë¥¼ ì†Œë¬¸ì ê·¸ëŒ€ë¡œ, ë§í¬Â·ì£¼ì„Â·ë°±í‹± ì—†ì´!)

    ë§Œì•½ í™•ì‹ í•  ìˆ˜ ì—†ìœ¼ë©´:
        {{ "pretrain_model": null }}
""").strip()


def gpt_find_pretrain(hf_id: str) -> str | None:
    ctx = _get_hf_context(hf_id)
    if not ctx:
        return None

    messages = [
        {"role": "system", "content": PROMPT_TMPL.format(hf_id=hf_id)},
        {"role": "user", "content": ctx}
    ]
    rsp = cli.chat.completions.create(
        model="gpt-4o-mini",
        response_format={"type": "json_object"},
        messages=messages,
        temperature=0
    )
    try:
        pred = json.loads(rsp.choices[0].message.content)
        return pred.get("pretrain_model")
    except Exception as e:
        print("âš ï¸ GPT ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨:", e)
        return None

# â”€â”€ CLI ì§„ì…ì  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("ì‚¬ìš©ë²•: python gpt_pretrain_finder.py <org/model>")
        sys.exit(1)

    model_id = sys.argv[1].strip()
    pre_id = gpt_find_pretrain(model_id)

    base = model_id.replace("/", "_")
    out = Path(f"pretrain_gpt_{base}.json")
    json.dump({"input_model": model_id, "pretrain_model": pre_id},
              open(out, "w", encoding="utf-8"), ensure_ascii=False, indent=2)

    if pre_id:
        print(f"âœ… GPT-4ê°€ ì¶”ì •í•œ í”„ë¦¬íŠ¸ë ˆì¸ ëª¨ë¸: {pre_id}")
    else:
        print("âš ï¸ GPT-4ê°€ í”„ë¦¬íŠ¸ë ˆì¸ ëª¨ë¸ì„ í™•ì‹ í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    print("ğŸ“ ê²°ê³¼ ì €ì¥:", out)
