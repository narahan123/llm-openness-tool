# inference.py
import os, sys, json, re, shlex, subprocess
from pathlib import Path
from typing import List, Dict, Any
from dotenv import load_dotenv
from openai import OpenAI

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í™˜ê²½
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()
_api_key = os.getenv("OPENAI_API_KEY")
if not _api_key:
    raise RuntimeError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
_client = OpenAI(api_key=_api_key)

OPENAI_MODEL = os.getenv("OPENAI_MODEL_INFER_PICK", "gpt-4o-mini")
ENC_RUN = "cp949" if os.name == "nt" else "utf-8"  # ì½˜ì†” ì¶œë ¥ ì¸ì½”ë”©

# ëª¨ë¸ íŒŒì´í”„ë¼ì¸ì—ì„œ ë‚´ë ¤ì£¼ëŠ” ê¸°ë³¸ ì¶œë ¥ í´ë” (ì—†ìœ¼ë©´ í˜„ì¬ í´ë”)
_DEFAULT_OUTDIR = os.getenv("MODEL_OUTPUT_DIR") or os.getenv("CURRENT_MODEL_DIR") or "."

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# GPT í”„ë¡¬í”„íŠ¸: ì‹¤í–‰ ì˜ˆì œ + pip ì„¤ì¹˜ ëª…ë ¹ì„ JSONìœ¼ë¡œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_SYSTEM = """
ë„ˆëŠ” Hugging Face README ë¶„ì„ ë„ìš°ë¯¸ë‹¤.
ëª©í‘œ: "ë¡œì»¬ì—ì„œ ë°”ë¡œ ëŒë¦´ ìˆ˜ ìˆëŠ” ë‹¨ì¼ íŒŒì´ì¬ ì˜ˆì œ" 1ê°œì™€, ì‹¤í–‰ ì „ì— í•„ìš”í•œ pip ì„¤ì¹˜ ëª…ë ¹ì„ ë½‘ì•„ë¼.

ê·œì¹™:
- ì˜¤ì§ JSON ê°ì²´ë§Œ ì¶œë ¥í•œë‹¤.
- ì„œë²„/REST API/vLLM ì„œë²„ ë„ìš°ê¸° ë“±ì€ ì œì™¸í•˜ê³ , "ë¡œì»¬ íŒŒì´ì¬ ìŠ¤í¬ë¦½íŠ¸" ì˜ˆì œë¥¼ ìš°ì„ í•œë‹¤.
- ê°€ëŠ¥í•˜ë©´ transformers ê¸°ë°˜ì˜ ê°„ë‹¨í•œ chat/inference ì˜ˆì œë¥¼ íƒí•œë‹¤.
- ì½”ë“œ ë¸”ë¡ í‘œì‹œëŠ” ì“°ì§€ ë§ê³ , code í•„ë“œì— ìˆœìˆ˜ ì½”ë“œë§Œ ë„£ì–´ë¼.
- ì„¤ì¹˜ ëª…ë ¹ì€ 'pip install ...' í˜•íƒœë¡œë§Œ ë„£ì–´ë¼ (ì—¬ëŸ¬ ì¤„ ê°€ëŠ¥).
- READMEì— ì„¤ì¹˜ ëª…ë ¹ì´ ì—†ë”ë¼ë„, ì½”ë“œì— í•„ìš”í•œ ìµœì†Œ ë¼ì´ë¸ŒëŸ¬ë¦¬(ì˜ˆ: transformers)ê°€ ë³´ì´ë©´ í¬í•¨í•´ë¼.
- torch ì„¤ì¹˜ëŠ” í™˜ê²½ ì˜ì¡´ì„±ì´ í¬ë¯€ë¡œ ì½”ë“œì—ì„œ ì‹¤ì œë¡œ í•„ìš”í•  ë•Œë§Œ ë„£ì–´ë¼.

ì¶œë ¥ JSON ìŠ¤í‚¤ë§ˆ:
{
  "pip_installs": ["pip install transformers>=4.46.0", ...],
  "filename": "run_tmp.py",
  "code_language": "python",
  "code": "<ì—¬ê¸°ì— íŒŒì´ì¬ ì½”ë“œ ì›ë¬¸>",
  "notes": "ì„ íƒ. ê°„ë‹¨í•œ ë¹„ê³ "
}
"""

def _ask_plan_from_readme(readme: str) -> Dict[str, Any]:
    resp = _client.chat.completions.create(
        model=OPENAI_MODEL,
        response_format={"type": "json_object"},
        messages=[
            {"role": "system", "content": _SYSTEM},
            {"role": "user", "content": readme},
        ],
        temperature=0  # ì¼ë¶€ ëª¨ë¸ì´ ë¯¸ì§€ì›ì´ë©´ ì œê±°í•´ë„ ë¨
    )
    try:
        return json.loads(resp.choices[0].message.content)
    except Exception:
        return {}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì„¤ì¹˜ ëª…ë ¹ ì •ê·œí™” & ì‹¤í–‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _normalize_pip_cmd(cmd: str) -> List[str] | None:
    """
    'pip install ...' / 'pip3 install ...' / 'python -m pip install ...' ë“±ì„
    í˜„ì¬ íŒŒì´ì¬ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•œ ëª…ë ¹ ë°°ì—´ë¡œ í‘œì¤€í™”.
    """
    if not isinstance(cmd, str):
        return None
    cmd = cmd.strip().lstrip("!").replace("pip3", "pip")
    m = re.search(r"(?:python\s*-m\s+)?pip\s+install\s+(.+)", cmd, flags=re.I)
    if not m:
        return None
    args = shlex.split(m.group(1))
    return [sys.executable, "-m", "pip", "install", "--disable-pip-version-check"] + args

def _ensure_minimal_installs(plan: Dict[str, Any]) -> List[str]:
    """
    READMEì— ì„¤ì¹˜ ëª…ë ¹ì´ ë¹„ì–´ìˆê±°ë‚˜ ë¶€ì¡±í•  ë•Œ ìµœì†Œ ë³´ì •.
    code ë‚´ìš©ì—ì„œ import í”ì ì„ ë³´ê³  transformers/torch ì¶”ê°€.
    """
    installs = [c for c in (plan.get("pip_installs") or []) if isinstance(c, str)]
    code = plan.get("code") or ""
    if "transformers" in code and not any("transformers" in c for c in installs):
        installs.append("pip install transformers>=4.46.0")
    if re.search(r"\bimport\s+torch\b|\btorch\.", code) and not any(c.strip().startswith("pip install torch") for c in installs):
        installs.append("pip install torch")
    return installs

def _run_cmd(cmd_list: List[str], cwd: Path | None = None, timeout: int | None = None) -> Dict[str, Any]:
    try:
        proc = subprocess.run(
            cmd_list,
            cwd=str(cwd) if cwd else None,
            capture_output=True,
            text=True,
            encoding=ENC_RUN,
            timeout=timeout
        )
        return {
            "cmd": " ".join(cmd_list),
            "returncode": proc.returncode,
            "stdout": proc.stdout,
            "stderr": proc.stderr
        }
    except subprocess.TimeoutExpired as e:
        return {
            "cmd": " ".join(cmd_list),
            "returncode": -9,
            "stdout": e.stdout or "",
            "stderr": f"TimeoutExpired: {e}"
        }
    except Exception as e:
        return {
            "cmd": " ".join(cmd_list),
            "returncode": -1,
            "stdout": "",
            "stderr": f"{type(e).__name__}: {e}"
        }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì•ˆì „í•œ ì¶œë ¥ í´ë” ê²°ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _safe_outdir(output_dir: str | Path | None) -> Path:
    """
    - ëª…ì‹œëœ output_dirì´ ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ ê¸°ë³¸(_DEFAULT_OUTDIR)
    - ì‹¤ìˆ˜ë¡œ ê¸´ ë¬¸ì¥/í”„ë¡¬í”„íŠ¸ê°€ ë„˜ì–´ì˜¤ë©´ ë¬´ì‹œí•˜ê³  ê¸°ë³¸ í´ë” ì‚¬ìš©
    """
    if output_dir is None:
        return Path(_DEFAULT_OUTDIR)

    p = Path(output_dir)
    name = p.name
    # ê³µë°± ê³¼ë‹¤/íŠ¹ìˆ˜ë¬¸ì ë§ìœ¼ë©´ í”„ë¡¬í”„íŠ¸ë¡œ ì˜¤ì¸ â†’ ê¸°ë³¸ í´ë” ì‚¬ìš©
    if len(name) > 48 or re.search(r"[\\/:*?\"<>|]", name) or len(re.findall(r"\s", name)) > 6:
        return Path(_DEFAULT_OUTDIR)
    return p

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸: ê³„íš ì¶”ì¶œ â†’ ì„¤ì¹˜ â†’ ì½”ë“œ ì‹¤í–‰ â†’ ê²°ê³¼ JSON ì €ì¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_inference(readme: str, output_dir: str | Path | None = None) -> Path:
    outdir = _safe_outdir(output_dir)
    outdir.mkdir(parents=True, exist_ok=True)

    # 1) GPTì—ê²Œ ì‹¤í–‰ ê³„íš ìš”ì²­
    plan = _ask_plan_from_readme(readme)
    if not plan or not isinstance(plan, dict):
        plan = {}

    # í•„ìˆ˜ í•„ë“œ ê¸°ë³¸ê°’ ë³´ì •
    filename = plan.get("filename") or "run_tmp.py"
    code = (plan.get("code") or "").strip()
    plan["filename"] = filename
    plan.setdefault("code_language", "python")
    plan.setdefault("pip_installs", [])
    plan.setdefault("notes", "")

    # 2) ì„¤ì¹˜ ëª…ë ¹ ë³´ì •/ì¶”ê°€
    plan["pip_installs"] = _ensure_minimal_installs(plan)

    # 3) ê³„íš JSON ì €ì¥ (ëª¨ë¸ í´ë”)
    plan_path = outdir / "inference_plan.json"
    with open(plan_path, "w", encoding="utf-8") as f:
        json.dump(plan, f, ensure_ascii=False, indent=2)

    install_logs: List[Dict[str, Any]] = []

    # 4) ì˜ì¡´ì„± ì„¤ì¹˜ ì‹¤í–‰ (ì‘ì—… ë””ë ‰í† ë¦¬ = ëª¨ë¸ í´ë”)
    for raw in plan["pip_installs"]:
        norm = _normalize_pip_cmd(raw)
        if not norm:
            install_logs.append({
                "cmd": raw, "returncode": -1, "stdout": "", "stderr": "Unrecognized pip command"
            })
            continue
        print(f"ğŸ“¦ Installing: {' '.join(norm)}")
        log = _run_cmd(norm, cwd=outdir, timeout=1200)  # ìµœëŒ€ 20ë¶„
        install_logs.append(log)

    # 5) ì½”ë“œ íŒŒì¼ ìƒì„± (ëª¨ë¸ í´ë”)
    codefile = outdir / filename
    created_code = False
    if code:
        with open(codefile, "w", encoding="utf-8") as f:
            f.write(code)
        created_code = True
        print(f"âœ… 3. ì¶”ì¶œëœ ì½”ë“œë¥¼ '{codefile.name}' íŒŒì¼ë¡œ ì„±ê³µì ìœ¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    else:
        print("âš ï¸ ì‹¤í–‰ ì½”ë“œê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. (READMEì— ë¡œì»¬ ì‹¤í–‰ ì˜ˆì œê°€ ì—†ì„ ìˆ˜ ìˆìŒ)")

    # 6) ì½”ë“œ ì‹¤í–‰ (ì‘ì—… ë””ë ‰í† ë¦¬ = ëª¨ë¸ í´ë”)
    exec_log = {"cmd": "", "returncode": None, "stdout": "", "stderr": ""}
    if created_code:
        print(f"\nâœ… 4. '{codefile.name}' ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        print("   (ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì‹¤í–‰ì— ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)\n")
        exec_cmd = [sys.executable, filename]
        exec_log = _run_cmd(exec_cmd, cwd=outdir, timeout=1800)  # ìµœëŒ€ 30ë¶„
        # ì‹¤í–‰ í›„ ì„ì‹œ íŒŒì¼ ì œê±°
        try:
            codefile.unlink(missing_ok=True)
            print(f"\nğŸ§¹ '{codefile.name}' ì„ì‹œ íŒŒì¼ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
        except Exception:
            pass

    # 7) ê²°ê³¼ JSON ì €ì¥ (ëª¨ë¸ í´ë”)
    result = {
        "plan_path": str(plan_path),
        "plan": plan,
        "installs": install_logs,
        "execution": exec_log,
    }
    result_path = outdir / "inference_result.json"
    with open(result_path, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    # ì½˜ì†” ìš”ì•½
    print("\n" + "-"*60)
    print(f"ğŸ“„ ê³„íš JSON: {plan_path}")
    print(f"ğŸ“„ ê²°ê³¼ JSON: {result_path}")
    print(f"â–¶ ì‹¤í–‰ ë°˜í™˜ì½”ë“œ: {exec_log.get('returncode')}")
    print("-"*60)

    return result_path

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë‹¨ë… ì‹¤í–‰ í…ŒìŠ¤íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


if __name__ == "__main__":
    # ë°ëª¨: README ë¬¸ìì—´ì„ ë°›ì•„ì„œ ì‹¤í–‰
    demo_readme = "ì—¬ê¸°ì— README í…ìŠ¤íŠ¸ë¥¼ ë„£ìœ¼ì„¸ìš”."
    run_inference(demo_readme, output_dir=".")


# if __name__=="__main__":
#     readme = """
#     ---\nlicense: apache-2.0\nlicense_link: https://huggingface.co/skt/A.X-4.0-Light/blob/main/LICENSE\nlanguage:\n- en\n- ko\npipeline_tag: text-generation\nlibrary_name: transformers\nmodel_id: skt/A.X-4.0-Light\ndevelopers: SKT AI Model Lab\nmodel-index:\n- name: A.X-4.0-Light\n  results:\n  - task:\n      type: generate_until\n      name: mmlu\n    dataset:\n      name: mmlu (chat CoT)\n      type: hails/mmlu_no_train\n    metrics:\n    - type: exact_match\n      value: 75.43\n      name: exact_match\n  - task:\n      type: generate_until\n      name: kmmlu\n    dataset:\n      name: kmmlu (chat CoT)\n      type: HAERAE-HUB/KMMLU\n    metrics:\n    - type: exact_match\n      value: 64.15\n      name: exact_match\n---\n\n# A.X 4.0 Light\n\n<p align=\"center\">\n    <picture>\n        <img src=\"./assets/A.X_logo_ko_4x3.png\" width=\"45%\" style=\"margin: 40px auto;\">\n    </picture>\n</p>\n<p align=\"center\"> <a href=\"https://huggingface.co/collections/skt/ax-4-68637ebaa63b9cc51925e886\">ğŸ¤— Models</a>   |   <a href=\"https://sktax.chat/chat\">ğŸ’¬ Chat</a>   |    <a href=\"https://github.com/SKT-AI/A.X-4.0/blob/main/apis/README.md\">ğŸ“¬ APIs (FREE!)</a>    |   <a href=\"https://github.com/SKT-AI/A.X-4.0\">ğŸ–¥ï¸ Github</a> </p>\n\n## A.X 4.0 Family Highlights\n\nSK Telecom released **A.X 4.0** (pronounced \"A dot X\"), a large language model (LLM) optimized for Korean-language understanding and enterprise deployment, on July 03, 2025. Built on the open-source [Qwen2.5](https://huggingface.co/collections/Qwen/qwen25-66e81a666513e518adb90d9e) model, A.X 4.0 has been further trained with large-scale Korean datasets to deliver outstanding performance in real-world business environments.\n\n- **Superior Korean Proficiency**: Achieved a score of 78.3 on [KMMLU](https://huggingface.co/datasets/HAERAE-HUB/KMMLU), the leading benchmark for Korean-language evaluation and a Korean-specific adaptation of MMLU, outperforming GPT-4o (72.5).\n- **Deep Cultural Understanding**: Scored 83.5 on [CLIcK](https://huggingface.co/datasets/EunsuKim/CLIcK), a benchmark for Korean cultural and contextual comprehension, surpassing GPT-4o (80.2).\n- **Efficient Token Usage**: A.X 4.0 uses approximately 33% fewer tokens than GPT-4o for the same Korean input, enabling more cost-effective and efficient processing.\n- **Deployment Flexibility**: Offered in both a 72B-parameter standard model (A.X 4.0) and a 7B lightweight version (A.X 4.0 Light).\n- **Long Context Handling**: Supports up to 131,072 tokens, allowing comprehension of lengthy documents and conversations. (Lightweight model supports up to 16,384 tokens length)\n\n## Performance\n\n### Model Performance\n\n<table><thead>\n  <tr>\n    <th colspan=\"2\">Benchmarks</th>\n    <th>A.X 4.0</th>\n    <th>Qwen3-235B-A22B<br/>(w/o reasoning)</th>\n    <th>Qwen2.5-72B</th>\n    <th>GPT-4o</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td rowspan=\"6\">Knowledge</td>\n    <td>KMMLU</td>\n    <td>78.32</td>\n    <td>73.64</td>\n    <td>66.44</td>\n    <td>72.51</td>\n  </tr>\n  <tr>\n    <td>KMMLU-pro</td>\n    <td>72.43</td>\n    <td>64.4</td>\n    <td>56.27</td>\n    <td>66.97</td>\n  </tr>\n  <tr>\n    <td>KMMLU-redux</td>\n    <td>74.18</td>\n    <td>71.17</td>\n    <td>58.76</td>\n    <td>69.08</td>\n  </tr>\n  <tr>\n    <td>CLIcK</td>\n    <td>83.51</td>\n    <td>74.55</td>\n    <td>72.59</td>\n    <td>80.22</td>\n  </tr>\n  <tr>\n    <td>KoBALT</td>\n    <td>47.30</td>\n    <td>41.57</td>\n    <td>37.00</td>\n    <td>44.00</td>\n  </tr>\n  <tr>\n    <td>MMLU</td>\n    <td>86.62</td>\n    <td>87.37</td>\n    <td>85.70</td>\n    <td>88.70</td>\n  </tr>\n  <tr>\n    <td rowspan=\"3\">General</td>\n    <td>Ko-MT-Bench</td>\n    <td>86.69</td>\n    <td>88.00</td>\n    <td>82.69</td>\n    <td>88.44</td>\n  </tr>\n  <tr>\n    <td>MT-Bench</td>\n    <td>83.25</td>\n    <td>86.56</td>\n    <td>93.50</td>\n    <td>88.19</td>\n  </tr>\n  <tr>\n    <td>LiveBench<sup>2024.11</sup></td>\n    <td>52.30</td>\n    <td>64.50</td>\n    <td>54.20</td>\n    <td>52.19</td>\n  </tr>\n  <tr>\n    <td rowspan=\"2\">Instruction Following</td>\n    <td>Ko-IFEval</td>\n    <td>77.96</td>\n    <td>77.53</td>\n    <td>77.07</td>\n    <td>75.38</td>\n  </tr>\n  <tr>\n    <td>IFEval</td>\n    <td>86.05</td>\n    <td>85.77</td>\n    <td>86.54</td>\n    <td>83.86</td>\n  </tr>\n  <tr>\n    <td rowspan=\"2\">Math</td>\n    <td>HRM8K</td>\n    <td>48.55</td>\n    <td>54.52</td>\n    <td>46.37</td>\n    <td>43.27</td>\n  </tr>\n  <tr>\n    <td>MATH</td>\n    <td>74.28</td>\n    <td>72.72</td>\n    <td>77.00</td>\n    <td>72.38</td>\n  </tr>\n  <tr>\n    <td rowspan=\"3\">Code</td>\n    <td>HumanEval+</td>\n    <td>79.27</td>\n    <td>79.27</td>\n    <td>81.71</td>\n    <td>86.00</td>\n  </tr>\n  <tr>\n    <td>MBPP+</td>\n    <td>73.28</td>\n    <td>70.11</td>\n    <td>75.66</td>\n    <td>75.10</td>\n  </tr>\n  <tr>\n    <td>LiveCodeBench<sup>2024.10~2025.04</sup></td>\n    <td>26.07</td>\n    <td>33.09</td>\n    <td>27.58</td>\n    <td>29.30</td>\n  </tr>\n  <tr>\n    <td>Long Context</td>\n    <td>LongBench<sup>&lt;128K</sup></td>\n    <td>56.70</td>\n    <td>49.40</td>\n    <td>45.60</td>\n    <td>47.50</td>\n  </tr>\n  <tr>\n    <td>Tool-use</td>\n    <td>FunctionChatBench</td>\n    <td>85.96</td>\n    <td>82.43</td>\n    <td>88.30</td>\n    <td>95.70</td>\n  </tr>\n</tbody></table>\n\n### Lightweight Model Performance\n\n<table><thead>\n  <tr>\n    <th colspan=\"2\">Benchmarks</th>\n    <th>A.X 4.0 Light</th>\n    <th>Qwen3-8B<br/>(w/o reasoning)</th>\n    <th>Qwen2.5-7B</th>\n    <th>EXAONE-3.5-7.8B</th>\n    <th>Kanana-1.5-8B</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td rowspan=\"6\">Knowledge</td>\n    <td>KMMLU</td>\n    <td>64.15</td>\n    <td>63.53</td>\n    <td>49.56</td>\n    <td>53.76</td>\n    <td>48.28</td>\n  </tr>\n  <tr>\n    <td>KMMLU-pro</td>\n    <td>50.28</td>\n    <td>50.71</td>\n    <td>38.87</td>\n    <td>40.11</td>\n    <td>37.63</td>\n  </tr>\n  <tr>\n    <td>KMMLU-redux</td>\n    <td>56.05</td>\n    <td>55.74</td>\n    <td>38.58</td>\n    <td>42.21</td>\n    <td>35.33</td>\n  </tr>\n  <tr>\n    <td>CLIcK</td>\n    <td>68.05</td>\n    <td>62.71</td>\n    <td>60.56</td>\n    <td>64.30</td>\n    <td>61.30</td>\n  </tr>\n  <tr>\n    <td>KoBALT</td>\n    <td>30.29</td>\n    <td>26.57</td>\n    <td>21.57</td>\n    <td>21.71</td>\n    <td>23.14</td>\n  </tr>\n  <tr>\n    <td>MMLU</td>\n    <td>75.43</td>\n    <td>82.89</td>\n    <td>75.40</td>\n    <td>72.20</td>\n    <td>68.82</td>\n  </tr>\n  <tr>\n    <td rowspan=\"3\">General</td>\n    <td>Ko-MT-Bench</td>\n    <td>79.50</td>\n    <td>64.06</td>\n    <td>61.31</td>\n    <td>81.06</td>\n    <td>76.30</td>\n  </tr>\n  <tr>\n    <td>MT-Bench</td>\n    <td>81.56</td>\n    <td>65.69</td>\n    <td>79.37</td>\n    <td>83.50</td>\n    <td>77.60</td>\n  </tr>\n  <tr>\n    <td>LiveBench</td>\n    <td>37.10</td>\n    <td>50.20</td>\n    <td>37.00</td>\n    <td>40.20</td>\n    <td>29.40</td>\n  </tr>\n  <tr>\n    <td rowspan=\"2\">Instruction Following</td>\n    <td>Ko-IFEval</td>\n    <td>72.99</td>\n    <td>73.39</td>\n    <td>60.73</td>\n    <td>65.01</td>\n    <td>69.96</td>\n  </tr>\n  <tr>\n    <td>IFEval</td>\n    <td>84.68</td>\n    <td>85.38</td>\n    <td>76.73</td>\n    <td>82.61</td>\n    <td>80.11</td>\n  </tr>\n  <tr>\n    <td rowspan=\"2\">Math</td>\n    <td>HRM8K</td>\n    <td>40.12</td>\n    <td>52.50</td>\n    <td>35.13</td>\n    <td>31.88</td>\n    <td>30.87</td>\n  </tr>\n  <tr>\n    <td>MATH</td>\n    <td>68.88</td>\n    <td>71.48</td>\n    <td>65.58</td>\n    <td>63.20</td>\n    <td>59.28</td>\n  </tr>\n  <tr>\n    <td rowspan=\"3\">Code</td>\n    <td>HumanEval+</td>\n    <td>75.61</td>\n    <td>77.44</td>\n    <td>74.39</td>\n    <td>76.83</td>\n    <td>76.83</td>\n  </tr>\n  <tr>\n    <td>MBPP+</td>\n    <td>67.20</td>\n    <td>62.17</td>\n    <td>68.50</td>\n    <td>64.29</td>\n    <td>67.99</td>\n  </tr>\n  <tr>\n    <td>LiveCodeBench</td>\n    <td>18.03</td>\n    <td>23.93</td>\n    <td>16.62</td>\n    <td>17.98</td>\n    <td>16.52</td>\n  </tr>\n</tbody></table>\n\n## ğŸš€ Quickstart\n\n### with HuggingFace Transformers\n\n- `transformers>=4.46.0` or the latest version is required to use `skt/A.X-4.0-Light`\n```bash\npip install transformers>=4.46.0\n```\n\n#### Example Usage\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"skt/A.X-4.0-Light\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\nmodel.eval()\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"ë‹¹ì‹ ì€ ì‚¬ìš©ìê°€ ì œê³µí•˜ëŠ” ì˜ì–´ ë¬¸ì¥ë“¤ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ëŠ” AI ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\"},\n    {\"role\": \"user\", \"content\": \"The first human went into space and orbited the Earth on April 12, 1961.\"},\n]\ninput_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n\nwith torch.no_grad():\n    output = model.generate(\n        input_ids,\n        max_new_tokens=128,\n        do_sample=False,\n    )\n\nlen_input_prompt = len(input_ids[0])\nresponse = tokenizer.decode(output[0][len_input_prompt:], skip_special_tokens=True)\nprint(response)\n# Output:\n# 1961ë…„ 4ì›” 12ì¼, ìµœì´ˆì˜ ì¸ê°„ì´ ìš°ì£¼ë¡œ ë‚˜ê°€ ì§€êµ¬ë¥¼ ê³µì „í–ˆìŠµë‹ˆë‹¤.\n```\n\n### with vLLM\n\n- `vllm>=v0.6.4.post1` or the latest version is required to use tool-use function\n```bash\npip install vllm>=v0.6.4.post1\n# if you don't want to activate tool-use function, just commenting out below vLLM option\nVLLM_OPTION=\"--enable-auto-tool-choice --tool-call-parser hermes\"\nvllm serve skt/A.X-4.0-Light $VLLM_OPTION\n```\n\n#### Example Usage \n  \n```python\nfrom openai import OpenAI\n\ndef call(messages, model):\n    completion = client.chat.completions.create(\n        model=model,\n        messages=messages,\n    )\n    print(completion.choices[0].message)\n\nclient = OpenAI(\n    base_url=\"http://localhost:8000/v1\",\n    api_key=\"api_key\"\n)\nmodel = \"skt/A.X-4.0-Light\"\nmessages = [{\"role\": \"user\", \"content\": \"ì—ì–´ì»¨ ì—¬ë¦„ì²  ì ì • ì˜¨ë„ëŠ”? í•œì¤„ë¡œ ë‹µë³€í•´ì¤˜\"}]\ncall(messages, model)\n# Output:\n# ChatCompletionMessage(content='ì—¬ë¦„ì²  ì ì • ì—ì–´ì»¨ ì˜¨ë„ëŠ” ì¼ë°˜ì ìœ¼ë¡œ 24-26ë„ì…ë‹ˆë‹¤.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], reasoning_content=None)\n\nmessages = [{\"role\": \"user\", \"content\": \"What is the appropriate temperature for air conditioning in summer? Response in a single sentence.\"}]\ncall(messages, model)\n# Output:\n# ChatCompletionMessage(content='The appropriate temperature for air conditioning in summer generally ranges from 72Â°F to 78Â°F (22Â°C to 26Â°C) for comfort and energy efficiency.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], reasoning_content=None)\n```\n\n#### Examples for tool-use\n```python\nfrom openai import OpenAI\n\n\ndef call(messages, model):\n    completion = client.chat.completions.create(\n        model=model,\n        messages=messages,\n        tools=tools\n    )\n    print(completion.choices[0].message)\n\n\nclient = OpenAI(\n    base_url=\"http://localhost:8000/v1\",\n    api_key=\"api_key\"\n)\nmodel = \"skt/A.X-4.0-Light\"\n\ncalculate_discount = {\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"calculate_discount\",\n        \"description\": \"ì›ê°€ê²©ê³¼ í• ì¸ìœ¨(í¼ì„¼íŠ¸ ë‹¨ìœ„)ì„ ì…ë ¥ë°›ì•„ í• ì¸ëœ ê°€ê²©ì„ê³„ì‚°í•œë‹¤.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"original_price\": {\n                    \"type\": \"number\",\n                    \"description\": \"ìƒí’ˆì˜ ì›ë˜ ê°€ê²©\"\n                },\n                \"discount_percentage\": {\n                    \"type\": \"number\",\n                    \"description\": \"ì ìš©í•  í• ì¸ìœ¨(ì˜ˆ: 20% í• ì¸ì˜ ê²½ìš° 20ì„ ì…ë ¥)\"\n                }\n            },\n            \"required\": [\"original_price\", \"discount_percentage\"]\n        }\n    }\n}\nget_exchange_rate = {\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"get_exchange_rate\",\n        \"description\": \"ë‘ í†µí™” ê°„ì˜ í™˜ìœ¨ì„ ê°€ì ¸ì˜¨ë‹¤.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"base_currency\": {\n                    \"type\": \"string\",\n                    \"description\": \"The currency to convert from.\"\n                },\n                \"target_currency\": {\n                    \"type\": \"string\",\n                    \"description\": \"The currency to convert to.\"\n                }\n            },\n            \"required\": [\"base_currency\", \"target_currency\"]\n        }\n    }\n}\ntools = [calculate_discount, get_exchange_rate]\n\n### Slot filling ###\nmessages = [{\"role\": \"user\", \"content\": \"ìš°ë¦¬ê°€ ë­˜ ì‚¬ì•¼ë˜ëŠ”ë° ì›ë˜ 57600ì›ì¸ë° ì§ì›í• ì¸ ë°›ì„ ìˆ˜ ìˆê±°ë“ ? í• ì¸ê°€ì¢€ ê³„ì‚°í•´ì¤˜\"}]\ncall(messages, model)\n# Output:\n# ChatCompletionMessage(content='í• ì¸ìœ¨ì„ ì•Œë ¤ì£¼ì‹œê² ìŠµë‹ˆê¹Œ?', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], reasoning_content=None)\n\n\n### Function calling ###\nmessages = [\n    {\"role\": \"user\", \"content\": \"ìš°ë¦¬ê°€ ë­˜ ì‚¬ì•¼ë˜ëŠ”ë° ì›ë˜ 57600ì›ì¸ë° ì§ì›í• ì¸ ë°›ì„ ìˆ˜ ìˆê±°ë“ ? í• ì¸ê°€ì¢€ ê³„ì‚°í•´ì¤˜\"},\n    {\"role\": \"assistant\", \"content\": \"í• ì¸ìœ¨ì„ ì•Œë ¤ì£¼ì‹œê² ìŠµë‹ˆê¹Œ?\"},\n    {\"role\": \"user\", \"content\": \"15% í• ì¸ ë°›ì„ ìˆ˜ ìˆì–´.\"},\n]\ncall(messages, model)\n# Output: \n# ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='chatcmpl-tool-7778d1d9fca94bf2acbb44c79359502c', function=Function(arguments='{\"original_price\": 57600, \"discount_percentage\": 15}', name='calculate_discount'), type='function')], reasoning_content=None)\n\n\n### Completion ###\nmessages = [\n    {\"role\": \"user\", \"content\": \"ìš°ë¦¬ê°€ ë­˜ ì‚¬ì•¼ë˜ëŠ”ë° ì›ë˜ 57600ì›ì¸ë° ì§ì›í• ì¸ ë°›ì„ ìˆ˜ ìˆê±°ë“ ? í• ì¸ê°€ì¢€ ê³„ì‚°í•´ì¤˜\"},\n    {\"role\": \"assistant\", \"content\": \"í• ì¸ìœ¨ì„ ì•Œë ¤ì£¼ì‹œê² ìŠµë‹ˆê¹Œ?\"},\n    {\"role\": \"user\", \"content\": \"15% í• ì¸ ë°›ì„ ìˆ˜ ìˆì–´.\"},\n    {\"role\": \"tool\", \"tool_call_id\": \"random_id\", \"name\": \"calculate_discount\", \"content\": \"{\\\"original_price\\\": 57600, \\\"discount_percentage\\\": 15, \\\"discounted_price\\\": 48960.0}\"}\n]\ncall(messages, model)\n# Output: \n# ChatCompletionMessage(content='57600ì›ì˜ ìƒí’ˆì—ì„œ 15% í• ì¸ì„ ì ìš©í•˜ë©´, í• ì¸ëœ ê°€ê²©ì€ 48960ì›ì…ë‹ˆë‹¤.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], reasoning_content=None)\n```\n\n## License\n\nThe `A.X 4.0 Light` model is licensed under `Apache License 2.0`.\n\n## Citation\n```\n@article{SKTAdotX4Light,\n  title={A.X 4.0 Light},\n  author={SKT AI Model Lab},\n  year={2025},\n  url={https://huggingface.co/skt/A.X-4.0-Light}\n}\n```\n\n## Contact\n\n- Business & Partnership Contact: [a.x@sk.com](a.x@sk.com)
#     """
#     run_inference(readme)